{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6e66035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 12.666612 [-0.3219064] [-0.3308509]\n",
      "20 0.12056133 [0.79488105] [0.14864582]\n",
      "40 0.006285146 [0.90494514] [0.1858368]\n",
      "60 0.0047724913 [0.91897464] [0.18130969]\n",
      "80 0.0043259743 [0.92369306] [0.17318936]\n",
      "100 0.0039288416 [0.92736596] [0.16508824]\n",
      "120 0.0035682328 [0.9307876] [0.15733337]\n",
      "140 0.0032407332 [0.9340412] [0.14993963]\n",
      "160 0.00294328 [0.93714106] [0.14289302]\n",
      "180 0.002673138 [0.94009525] [0.1361776]\n",
      "200 0.0024277826 [0.9429106] [0.12977773]\n",
      "220 0.0022049604 [0.94559354] [0.12367868]\n",
      "240 0.0020025808 [0.94815034] [0.11786626]\n",
      "260 0.001818773 [0.95058715] [0.11232705]\n",
      "280 0.0016518425 [0.9529094] [0.10704806]\n",
      "300 0.0015002219 [0.9551225] [0.10201717]\n",
      "320 0.0013625313 [0.9572315] [0.09722275]\n",
      "340 0.0012374744 [0.95924157] [0.09265367]\n",
      "360 0.00112389 [0.961157] [0.08829924]\n",
      "380 0.0010207323 [0.96298254] [0.08414948]\n",
      "400 0.0009270467 [0.9647222] [0.08019474]\n",
      "420 0.0008419561 [0.9663801] [0.07642587]\n",
      "440 0.0007646785 [0.9679601] [0.07283413]\n",
      "460 0.00069449766 [0.96946585] [0.06941122]\n",
      "480 0.00063075236 [0.9709009] [0.06614912]\n",
      "500 0.0005728573 [0.97226846] [0.06304033]\n",
      "520 0.0005202806 [0.9735717] [0.06007766]\n",
      "540 0.00047252644 [0.9748138] [0.05725422]\n",
      "560 0.00042915755 [0.97599745] [0.05456347]\n",
      "580 0.0003897654 [0.97712547] [0.05199918]\n",
      "600 0.00035399207 [0.9782005] [0.04955541]\n",
      "620 0.00032149962 [0.9792249] [0.04722651]\n",
      "640 0.00029199195 [0.9802013] [0.04500705]\n",
      "660 0.00026519023 [0.98113185] [0.04289186]\n",
      "680 0.00024085038 [0.98201853] [0.04087609]\n",
      "700 0.00021874563 [0.9828636] [0.03895508]\n",
      "720 0.00019866874 [0.9836689] [0.03712436]\n",
      "740 0.00018043276 [0.9844364] [0.03537965]\n",
      "760 0.00016387373 [0.98516786] [0.03371695]\n",
      "780 0.00014882986 [0.98586494] [0.03213235]\n",
      "800 0.00013517226 [0.9865293] [0.03062223]\n",
      "820 0.00012276367 [0.9871623] [0.02918309]\n",
      "840 0.000111498106 [0.9877656] [0.02781161]\n",
      "860 0.00010126303 [0.98834056] [0.0265046]\n",
      "880 9.196839e-05 [0.98888844] [0.02525901]\n",
      "900 8.352895e-05 [0.9894106] [0.02407197]\n",
      "920 7.5862255e-05 [0.9899083] [0.02294075]\n",
      "940 6.889936e-05 [0.9903827] [0.02186265]\n",
      "960 6.257553e-05 [0.9908346] [0.02083514]\n",
      "980 5.683119e-05 [0.9912654] [0.01985593]\n",
      "1000 5.1615207e-05 [0.99167585] [0.01892273]\n",
      "1020 4.6877405e-05 [0.9920671] [0.0180334]\n",
      "1040 4.2574608e-05 [0.9924399] [0.01718589]\n",
      "1060 3.8666647e-05 [0.9927952] [0.0163782]\n",
      "1080 3.5118366e-05 [0.99313384] [0.01560847]\n",
      "1100 3.1894106e-05 [0.99345654] [0.01487489]\n",
      "1120 2.8967543e-05 [0.99376404] [0.01417582]\n",
      "1140 2.6308882e-05 [0.99405706] [0.01350962]\n",
      "1160 2.3893888e-05 [0.99433637] [0.01287472]\n",
      "1180 2.1700798e-05 [0.9946025] [0.01226966]\n",
      "1200 1.9709047e-05 [0.9948562] [0.01169304]\n",
      "1220 1.790004e-05 [0.995098] [0.01114351]\n",
      "1240 1.625678e-05 [0.99532837] [0.01061978]\n",
      "1260 1.4765285e-05 [0.9955479] [0.01012067]\n",
      "1280 1.34093925e-05 [0.9957571] [0.00964503]\n",
      "1300 1.2179166e-05 [0.99595654] [0.00919177]\n",
      "1320 1.1060979e-05 [0.99614656] [0.0087598]\n",
      "1340 1.0045883e-05 [0.99632764] [0.00834813]\n",
      "1360 9.124196e-06 [0.99650025] [0.0079558]\n",
      "1380 8.286328e-06 [0.9966647] [0.00758192]\n",
      "1400 7.525827e-06 [0.99682146] [0.00722557]\n",
      "1420 6.8349304e-06 [0.99697083] [0.00688599]\n",
      "1440 6.2076147e-06 [0.9971132] [0.00656238]\n",
      "1460 5.6378776e-06 [0.9972488] [0.00625397]\n",
      "1480 5.1202564e-06 [0.9973781] [0.00596007]\n",
      "1500 4.650438e-06 [0.9975014] [0.00567997]\n",
      "1520 4.2234155e-06 [0.9976188] [0.00541304]\n",
      "1540 3.8359967e-06 [0.99773073] [0.00515863]\n",
      "1560 3.483892e-06 [0.99783736] [0.0049162]\n",
      "1580 3.1642705e-06 [0.997939] [0.00468514]\n",
      "1600 2.8736388e-06 [0.99803585] [0.00446496]\n",
      "1620 2.6100854e-06 [0.9981282] [0.00425514]\n",
      "1640 2.3704606e-06 [0.9982161] [0.00405515]\n",
      "1660 2.1528751e-06 [0.99829996] [0.00386459]\n",
      "1680 1.9551626e-06 [0.9983798] [0.00368298]\n",
      "1700 1.7758848e-06 [0.99845594] [0.00350992]\n",
      "1720 1.6127851e-06 [0.9985285] [0.00334499]\n",
      "1740 1.4650474e-06 [0.9985976] [0.00318784]\n",
      "1760 1.330575e-06 [0.9986635] [0.00303806]\n",
      "1780 1.2083614e-06 [0.99872637] [0.00289531]\n",
      "1800 1.0974081e-06 [0.99878615] [0.00275926]\n",
      "1820 9.966674e-07 [0.9988432] [0.0026296]\n",
      "1840 9.053379e-07 [0.9988976] [0.00250603]\n",
      "1860 8.222731e-07 [0.9989494] [0.00238828]\n",
      "1880 7.467251e-07 [0.99899876] [0.00227605]\n",
      "1900 6.782545e-07 [0.9990458] [0.00216912]\n",
      "1920 6.1599695e-07 [0.9990906] [0.0020672]\n",
      "1940 5.594822e-07 [0.9991333] [0.00197008]\n",
      "1960 5.081961e-07 [0.99917406] [0.00187751]\n",
      "1980 4.6156163e-07 [0.9992128] [0.00178932]\n",
      "2000 4.1913586e-07 [0.9992498] [0.00170525]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# X and Y data\n",
    "x_train = [1, 2, 3]\n",
    "y_train = [1, 2, 3]\n",
    "\n",
    "W = tf.Variable(tf.random.normal([1]), name = 'weight') #[1]은 rank가 1임을 의미, W,b의 값을 모르기 때문에 random을 줌\n",
    "b = tf.Variable(tf.random.normal([1]), name = 'bias')\n",
    "# Our hypothesis WX + b\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "for step in range(2001):\n",
    "    #해당 블록 내에서 연산을 기록, 기록된 연산들의 gredients를 계산하는데 사용함\n",
    "    with tf.GradientTape() as tape:\n",
    "        hypothesis = x_train * W + b\n",
    "        cost = tf.reduce_mean(tf.square(hypothesis - y_train))\n",
    "\n",
    "    #W와 b의 기울기를 계산하여 저장   \n",
    "    W_grad, b_grad = tape.gradient(cost, [W, b])\n",
    "    \n",
    "    #기울기를 통해 W와 b를 업데이트 해줌\n",
    "    W.assign_sub(learning_rate * W_grad)\n",
    "    b.assign_sub(learning_rate * b_grad)\n",
    "    #20번에 한번씩 출력\n",
    "    if step % 20 == 0:\n",
    "        print(step, cost.numpy(), W.numpy(), b.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "61eb7bca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 10.97378 [0.33097687] [0.7615816]\n",
      "20 0.0034598983 [1.0336078] [0.96622646]\n",
      "40 0.0028261098 [1.0343827] [0.9758116]\n",
      "60 0.0024680584 [1.0321443] [0.98394847]\n",
      "80 0.0021553678 [1.0300391] [0.9915489]\n",
      "100 0.0018822914 [1.0280718] [0.99865156]\n",
      "120 0.0016438181 [1.0262333] [1.0052891]\n",
      "140 0.0014355659 [1.0245153] [1.0114917]\n",
      "160 0.0012536871 [1.0229099] [1.0172881]\n",
      "180 0.0010948608 [1.0214095] [1.022705]\n",
      "200 0.0009561478 [1.0200074] [1.0277671]\n",
      "220 0.00083501183 [1.018697] [1.0324976]\n",
      "240 0.00072922284 [1.0174726] [1.0369183]\n",
      "260 0.0006368372 [1.0163283] [1.0410495]\n",
      "280 0.0005561565 [1.0152589] [1.0449102]\n",
      "300 0.0004856952 [1.0142597] [1.048518]\n",
      "320 0.00042416397 [1.0133258] [1.0518894]\n",
      "340 0.00037042273 [1.0124531] [1.0550402]\n",
      "360 0.00032349234 [1.0116376] [1.0579848]\n",
      "380 0.00028250212 [1.0108752] [1.0607368]\n",
      "400 0.00024671343 [1.0101632] [1.0633081]\n",
      "420 0.000215456 [1.0094974] [1.0657111]\n",
      "440 0.00018815909 [1.0088755] [1.0679567]\n",
      "460 0.00016431633 [1.008294] [1.0700557]\n",
      "480 0.00014349878 [1.0077509] [1.0720167]\n",
      "500 0.00012531853 [1.0072433] [1.0738494]\n",
      "520 0.00010944104 [1.0067688] [1.0755621]\n",
      "540 9.557655e-05 [1.0063256] [1.0771625]\n",
      "560 8.346709e-05 [1.0059114] [1.0786581]\n",
      "580 7.289459e-05 [1.0055243] [1.0800555]\n",
      "600 6.365897e-05 [1.0051625] [1.0813617]\n",
      "620 5.559408e-05 [1.0048243] [1.0825825]\n",
      "640 4.8550137e-05 [1.0045084] [1.0837232]\n",
      "660 4.2399635e-05 [1.0042131] [1.0847892]\n",
      "680 3.7027952e-05 [1.0039372] [1.0857853]\n",
      "700 3.2336196e-05 [1.0036794] [1.0867162]\n",
      "720 2.8240622e-05 [1.0034385] [1.087586]\n",
      "740 2.4662702e-05 [1.0032133] [1.088399]\n",
      "760 2.1537564e-05 [1.0030028] [1.0891589]\n",
      "780 1.8810984e-05 [1.0028063] [1.0898682]\n",
      "800 1.6427652e-05 [1.0026225] [1.0905317]\n",
      "820 1.4346604e-05 [1.0024508] [1.0911518]\n",
      "840 1.252965e-05 [1.0022904] [1.0917312]\n",
      "860 1.0942337e-05 [1.0021404] [1.0922726]\n",
      "880 9.556187e-06 [1.0020002] [1.0927786]\n",
      "900 8.3454615e-06 [1.0018692] [1.0932515]\n",
      "920 7.2882494e-06 [1.0017468] [1.0936935]\n",
      "940 6.3648063e-06 [1.0016325] [1.0941066]\n",
      "960 5.5585724e-06 [1.0015255] [1.0944924]\n",
      "980 4.8544644e-06 [1.0014255] [1.094853]\n",
      "1000 4.2395673e-06 [1.0013323] [1.0951902]\n",
      "1020 3.702622e-06 [1.001245] [1.095505]\n",
      "1040 3.2332914e-06 [1.0011636] [1.0957993]\n",
      "1060 2.823944e-06 [1.0010874] [1.0960743]\n",
      "1080 2.4661338e-06 [1.0010161] [1.0963314]\n",
      "1100 2.153585e-06 [1.0009496] [1.0965718]\n",
      "1120 1.8806832e-06 [1.0008874] [1.0967963]\n",
      "1140 1.6426098e-06 [1.0008292] [1.097006]\n",
      "1160 1.4345806e-06 [1.0007751] [1.0972021]\n",
      "1180 1.2528408e-06 [1.0007242] [1.0973852]\n",
      "1200 1.0941897e-06 [1.000677] [1.0975562]\n",
      "1220 9.5568e-07 [1.0006326] [1.0977162]\n",
      "1240 8.3469115e-07 [1.0005913] [1.0978657]\n",
      "1260 7.291621e-07 [1.0005525] [1.0980053]\n",
      "1280 6.366989e-07 [1.0005163] [1.0981358]\n",
      "1300 5.562091e-07 [1.0004826] [1.0982579]\n",
      "1320 4.857051e-07 [1.000451] [1.0983719]\n",
      "1340 4.2424463e-07 [1.0004216] [1.0984783]\n",
      "1360 3.705644e-07 [1.000394] [1.0985779]\n",
      "1380 3.2375425e-07 [1.0003681] [1.0986708]\n",
      "1400 2.827192e-07 [1.0003442] [1.0987577]\n",
      "1420 2.4698983e-07 [1.0003216] [1.098839]\n",
      "1440 2.1562975e-07 [1.0003004] [1.0989151]\n",
      "1460 1.8836884e-07 [1.000281] [1.098986]\n",
      "1480 1.6452469e-07 [1.0002625] [1.0990523]\n",
      "1500 1.4369942e-07 [1.0002455] [1.0991143]\n",
      "1520 1.2549009e-07 [1.0002292] [1.0991724]\n",
      "1540 1.0967959e-07 [1.0002145] [1.0992262]\n",
      "1560 9.5782056e-08 [1.0002002] [1.0992769]\n",
      "1580 8.368988e-08 [1.0001873] [1.0993241]\n",
      "1600 7.311108e-08 [1.0001751] [1.0993682]\n",
      "1620 6.388797e-08 [1.0001634] [1.0994095]\n",
      "1640 5.580399e-08 [1.0001528] [1.0994482]\n",
      "1660 4.8752792e-08 [1.000143] [1.0994842]\n",
      "1680 4.261439e-08 [1.0001335] [1.0995177]\n",
      "1700 3.7193846e-08 [1.0001248] [1.0995493]\n",
      "1720 3.2539642e-08 [1.0001167] [1.0995786]\n",
      "1740 2.843907e-08 [1.0001092] [1.099606]\n",
      "1760 2.483902e-08 [1.000102] [1.0996318]\n",
      "1780 2.1708264e-08 [1.0000952] [1.0996556]\n",
      "1800 1.8979994e-08 [1.000089] [1.0996782]\n",
      "1820 1.6525329e-08 [1.0000832] [1.0996995]\n",
      "1840 1.44892285e-08 [1.000078] [1.0997187]\n",
      "1860 1.2670102e-08 [1.000073] [1.099737]\n",
      "1880 1.1114184e-08 [1.0000682] [1.0997537]\n",
      "1900 9.660584e-09 [1.0000634] [1.0997704]\n",
      "1920 8.462294e-09 [1.0000595] [1.099785]\n",
      "1940 7.379313e-09 [1.0000556] [1.0997993]\n",
      "1960 6.435781e-09 [1.000052] [1.0998124]\n",
      "1980 5.645222e-09 [1.0000488] [1.0998243]\n",
      "2000 4.9187436e-09 [1.0000454] [1.0998362]\n",
      "[6.1000633]\n",
      "[3.5999498]\n",
      "[2.5999045 4.599995 ]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "@tf.function\n",
    "def train_step(x_train, y_train):\n",
    "    with tf.GradientTape() as tape:\n",
    "        hypothesis = x_train * W + b\n",
    "        cost = tf.reduce_mean(tf.square(hypothesis - y_train))\n",
    "\n",
    "    gradients = tape.gradient(cost, [W, b])\n",
    "    #계산도니 기울기를 W와 b에 적용하여 변수를 업데이트 해줌\n",
    "    optimizer.apply_gradients(zip(gradients, [W, b]))\n",
    "    return cost\n",
    "\n",
    "@tf.function\n",
    "def hypothesis(x):\n",
    "    return x * W + b\n",
    "\n",
    "\n",
    "W = tf.Variable(tf.random.normal([1]), name='weight')\n",
    "b = tf.Variable(tf.random.normal([1]), name='bias')\n",
    "\n",
    "learning_rate = 0.01\n",
    "#SGD는 경상하강법 알고리즘을 사용하는 최적화 객체 생성 메소드\n",
    "optimizer = tf.optimizers.SGD(learning_rate)\n",
    "\n",
    "x_train = [1, 2, 3, 4, 5]\n",
    "y_train = [2.1, 3.1, 4.1, 5.1, 6.1]\n",
    "\n",
    "for step in range(2001):\n",
    "    cost_val = train_step(x_train, y_train)\n",
    "    if step % 20 == 0:\n",
    "        print(step, cost_val.numpy(), W.numpy(), b.numpy())\n",
    "\n",
    "#Testing our model\n",
    "#hypothesis 함수를 만들어 실행\n",
    "print(hypothesis([5]).numpy())\n",
    "print(hypothesis([2.5]).numpy())\n",
    "print(hypothesis([1.5, 3.5]).numpy())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
