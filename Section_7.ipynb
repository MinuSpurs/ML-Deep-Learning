{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "1 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "2 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "3 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "4 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "5 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "6 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "7 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "8 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "9 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "10 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "11 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "12 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "13 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "14 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "15 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "16 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "17 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "18 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "19 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "20 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "21 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "22 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "23 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "24 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "25 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "26 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "27 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "28 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "29 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "30 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "31 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "32 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "33 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "34 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "35 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "36 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "37 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "38 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "39 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "40 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "41 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "42 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "43 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "44 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "45 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "46 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "47 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "48 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "49 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "50 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "51 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "52 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "53 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "54 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "55 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "56 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "57 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "58 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "59 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "60 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "61 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "62 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "63 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "64 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "65 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "66 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "67 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "68 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "69 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "70 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "71 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "72 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "73 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "74 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "75 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "76 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "77 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "78 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "79 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "80 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "81 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "82 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "83 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "84 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "85 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "86 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "87 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "88 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "89 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "90 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "91 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "92 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "93 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "94 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "95 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "96 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "97 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "98 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "99 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "100 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "101 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "102 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "103 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "104 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "105 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "106 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "107 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "108 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "109 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "110 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "111 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "112 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "113 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "114 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "115 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "116 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "117 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "118 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "119 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "120 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "121 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "122 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "123 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "124 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "125 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "126 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "127 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "128 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "129 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "130 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "131 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "132 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "133 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "134 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "135 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "136 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "137 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "138 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "139 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "140 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "141 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "142 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "143 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "144 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "145 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "146 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "147 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "148 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "149 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "150 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "151 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "152 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "153 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "154 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "155 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "156 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "157 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "158 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "159 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "160 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "161 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "162 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "163 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "164 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "165 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "166 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "167 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "168 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "169 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "170 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "171 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "172 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "173 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "174 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "175 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "176 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "177 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "178 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "179 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "180 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "181 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "182 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "183 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "184 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "185 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "186 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "187 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "188 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "189 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "190 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "191 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "192 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "193 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "194 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "195 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "196 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "197 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "198 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "199 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "200 -6.6711903 [[-0.4487237  -0.5047433  -0.18290754]\n",
      " [ 0.33017343  0.573457    1.7419223 ]\n",
      " [-0.9023333  -0.5533279  -0.26609644]]\n",
      "Prediction: [2 2 2]\n",
      "Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "x_data = [[1, 2, 1],\n",
    "          [1, 3, 2],\n",
    "          [1, 3, 4],\n",
    "          [1, 5, 5],\n",
    "          [1, 7, 5],\n",
    "          [1, 2, 5],\n",
    "          [1, 6, 6],\n",
    "          [1, 7, 7]]\n",
    "y_data = [[0, 0, 1],\n",
    "          [0, 0, 1],\n",
    "          [0, 0, 1],\n",
    "          [0, 1, 0],\n",
    "          [0, 1, 0],\n",
    "          [0, 1, 0],\n",
    "          [1, 0, 0],\n",
    "          [1, 0, 0]]\n",
    "\n",
    "x_test = [[2, 1, 1],\n",
    "          [3, 1, 2],\n",
    "          [3, 3, 4]]\n",
    "y_test = [[0, 0, 1],\n",
    "          [0, 0, 1],\n",
    "          [0, 0, 1]]\n",
    "\n",
    "X = tf.constant(x_data, dtype=tf.float32)\n",
    "Y = tf.constant(y_data, dtype=tf.float32)\n",
    "\n",
    "W = tf.Variable(tf.random.normal([3, 3]))\n",
    "b = tf.Variable(tf.random.normal([3]))\n",
    "\n",
    "#hypothesis function\n",
    "def model(X, W, b):\n",
    "    return tf.nn.softmax(tf.matmul(X, W) + b)\n",
    "\n",
    "#cross enthropy\n",
    "def loss(Y, hypothesis):\n",
    "    return tf.reduce_mean(tf.reduce_sum(Y * tf.math.log(hypothesis), axis=1))\n",
    "\n",
    "def train(X, Y, W, b, learning_rate=1e-10):\n",
    "    with tf.GradientTape() as tape:\n",
    "        hypothesis = model(X, W, b)\n",
    "        cost = loss(Y, hypothesis)\n",
    "        \n",
    "    gradients = tape.gradient(cost, [W, b])\n",
    "    \n",
    "    W.assign_sub(learning_rate * gradients[0])\n",
    "    b.assign_sub(learning_rate * gradients[1])\n",
    "\n",
    "def predict(X, W, b):\n",
    "    return tf.argmax(model(X, W, b), 1)\n",
    "\n",
    "def accuracy(X, Y, W, b):\n",
    "    prediction = predict(X, W, b)\n",
    "    is_correct = tf.equal(prediction, tf.argmax(Y, 1))\n",
    "    return tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "\n",
    "for step in range(201):\n",
    "    train(X, Y, W, b)\n",
    "    cost_val = loss(Y, model(X, W, b))\n",
    "    print(step, cost_val.numpy(), W.numpy())\n",
    "\n",
    "x_test_var = tf.constant(x_test, dtype=tf.float32)\n",
    "print(\"Prediction:\", predict(x_test_var, W, b).numpy())\n",
    "print(\"Accuracy: \", accuracy(x_test_var, tf.constant(y_test, dtype=tf.float32), W, b).numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Cost:  6.2721004 \n",
      "Prediction:\n",
      " [[-0.96732867]\n",
      " [-2.0478768 ]\n",
      " [-1.9956377 ]\n",
      " [-1.9316243 ]\n",
      " [-1.8857651 ]\n",
      " [-1.9595622 ]\n",
      " [-2.391622  ]\n",
      " [-2.7030509 ]]\n",
      "1 Cost:  6.2716575 \n",
      "Prediction:\n",
      " [[-0.9672111]\n",
      " [-2.0477586]\n",
      " [-1.9955385]\n",
      " [-1.9315468]\n",
      " [-1.8856742]\n",
      " [-1.9594742]\n",
      " [-2.39156  ]\n",
      " [-2.7029886]]\n",
      "2 Cost:  6.2712145 \n",
      "Prediction:\n",
      " [[-0.96709335]\n",
      " [-2.04764   ]\n",
      " [-1.9954394 ]\n",
      " [-1.9314694 ]\n",
      " [-1.8855835 ]\n",
      " [-1.9593861 ]\n",
      " [-2.391498  ]\n",
      " [-2.7029262 ]]\n",
      "3 Cost:  6.270771 \n",
      "Prediction:\n",
      " [[-0.9669759]\n",
      " [-2.0475218]\n",
      " [-1.9953403]\n",
      " [-1.931392 ]\n",
      " [-1.8854927]\n",
      " [-1.9592981]\n",
      " [-2.391436 ]\n",
      " [-2.702864 ]]\n",
      "4 Cost:  6.270328 \n",
      "Prediction:\n",
      " [[-0.96685815]\n",
      " [-2.0474036 ]\n",
      " [-1.9952412 ]\n",
      " [-1.9313145 ]\n",
      " [-1.8854018 ]\n",
      " [-1.9592102 ]\n",
      " [-2.391374  ]\n",
      " [-2.7028017 ]]\n",
      "5 Cost:  6.269884 \n",
      "Prediction:\n",
      " [[-0.9667406]\n",
      " [-2.0472853]\n",
      " [-1.9951421]\n",
      " [-1.931237 ]\n",
      " [-1.8853111]\n",
      " [-1.9591221]\n",
      " [-2.3913121]\n",
      " [-2.7027395]]\n",
      "6 Cost:  6.2694416 \n",
      "Prediction:\n",
      " [[-0.9666228]\n",
      " [-2.047167 ]\n",
      " [-1.995043 ]\n",
      " [-1.9311595]\n",
      " [-1.8852203]\n",
      " [-1.959034 ]\n",
      " [-2.3912501]\n",
      " [-2.7026772]]\n",
      "7 Cost:  6.268998 \n",
      "Prediction:\n",
      " [[-0.9665054]\n",
      " [-2.0470488]\n",
      " [-1.9949439]\n",
      " [-1.931082 ]\n",
      " [-1.8851295]\n",
      " [-1.958946 ]\n",
      " [-2.3911881]\n",
      " [-2.7026148]]\n",
      "8 Cost:  6.2685547 \n",
      "Prediction:\n",
      " [[-0.9663876]\n",
      " [-2.0469303]\n",
      " [-1.9948448]\n",
      " [-1.9310045]\n",
      " [-1.8850387]\n",
      " [-1.958858 ]\n",
      " [-2.3911262]\n",
      " [-2.7025526]]\n",
      "9 Cost:  6.268111 \n",
      "Prediction:\n",
      " [[-0.9662702]\n",
      " [-2.046812 ]\n",
      " [-1.9947457]\n",
      " [-1.9309272]\n",
      " [-1.884948 ]\n",
      " [-1.9587699]\n",
      " [-2.3910642]\n",
      " [-2.7024903]]\n",
      "10 Cost:  6.267668 \n",
      "Prediction:\n",
      " [[-0.9661524]\n",
      " [-2.0466938]\n",
      " [-1.9946467]\n",
      " [-1.9308497]\n",
      " [-1.8848572]\n",
      " [-1.958682 ]\n",
      " [-2.3910022]\n",
      " [-2.702428 ]]\n",
      "11 Cost:  6.2672253 \n",
      "Prediction:\n",
      " [[-0.9660349]\n",
      " [-2.0465755]\n",
      " [-1.9945476]\n",
      " [-1.9307722]\n",
      " [-1.8847665]\n",
      " [-1.9585938]\n",
      " [-2.3909402]\n",
      " [-2.7023659]]\n",
      "12 Cost:  6.266782 \n",
      "Prediction:\n",
      " [[-0.9659172]\n",
      " [-2.0464573]\n",
      " [-1.9944485]\n",
      " [-1.9306948]\n",
      " [-1.8846756]\n",
      " [-1.9585059]\n",
      " [-2.3908782]\n",
      " [-2.7023036]]\n",
      "13 Cost:  6.266339 \n",
      "Prediction:\n",
      " [[-0.9657997]\n",
      " [-2.046339 ]\n",
      " [-1.9943495]\n",
      " [-1.9306172]\n",
      " [-1.8845849]\n",
      " [-1.9584179]\n",
      " [-2.390816 ]\n",
      " [-2.7022414]]\n",
      "14 Cost:  6.265896 \n",
      "Prediction:\n",
      " [[-0.9656819]\n",
      " [-2.0462208]\n",
      " [-1.9942503]\n",
      " [-1.9305398]\n",
      " [-1.8844941]\n",
      " [-1.9583299]\n",
      " [-2.390754 ]\n",
      " [-2.702179 ]]\n",
      "15 Cost:  6.2654524 \n",
      "Prediction:\n",
      " [[-0.9655645]\n",
      " [-2.0461025]\n",
      " [-1.9941512]\n",
      " [-1.9304624]\n",
      " [-1.8844032]\n",
      " [-1.9582418]\n",
      " [-2.390692 ]\n",
      " [-2.7021167]]\n",
      "16 Cost:  6.2650094 \n",
      "Prediction:\n",
      " [[-0.9654467]\n",
      " [-2.0459843]\n",
      " [-1.9940522]\n",
      " [-1.9303849]\n",
      " [-1.8843125]\n",
      " [-1.9581538]\n",
      " [-2.39063  ]\n",
      " [-2.7020545]]\n",
      "17 Cost:  6.2645664 \n",
      "Prediction:\n",
      " [[-0.9653293]\n",
      " [-2.045866 ]\n",
      " [-1.9939531]\n",
      " [-1.9303074]\n",
      " [-1.8842218]\n",
      " [-1.9580657]\n",
      " [-2.390568 ]\n",
      " [-2.7019923]]\n",
      "18 Cost:  6.264124 \n",
      "Prediction:\n",
      " [[-0.9652115]\n",
      " [-2.0457478]\n",
      " [-1.993854 ]\n",
      " [-1.93023  ]\n",
      " [-1.8841311]\n",
      " [-1.9579778]\n",
      " [-2.390506 ]\n",
      " [-2.70193  ]]\n",
      "19 Cost:  6.263681 \n",
      "Prediction:\n",
      " [[-0.9650941]\n",
      " [-2.0456297]\n",
      " [-1.993755 ]\n",
      " [-1.9301525]\n",
      " [-1.8840404]\n",
      " [-1.9578898]\n",
      " [-2.390444 ]\n",
      " [-2.7018678]]\n",
      "20 Cost:  6.263238 \n",
      "Prediction:\n",
      " [[-0.96497643]\n",
      " [-2.0455115 ]\n",
      " [-1.9936559 ]\n",
      " [-1.930075  ]\n",
      " [-1.8839495 ]\n",
      " [-1.9578018 ]\n",
      " [-2.390382  ]\n",
      " [-2.7018056 ]]\n",
      "21 Cost:  6.2627954 \n",
      "Prediction:\n",
      " [[-0.9648588]\n",
      " [-2.0453932]\n",
      " [-1.993557 ]\n",
      " [-1.9299977]\n",
      " [-1.8838588]\n",
      " [-1.9577138]\n",
      " [-2.39032  ]\n",
      " [-2.7017434]]\n",
      "22 Cost:  6.2623525 \n",
      "Prediction:\n",
      " [[-0.96474135]\n",
      " [-2.0452752 ]\n",
      " [-1.9934579 ]\n",
      " [-1.9299202 ]\n",
      " [-1.8837681 ]\n",
      " [-1.9576259 ]\n",
      " [-2.390258  ]\n",
      " [-2.7016811 ]]\n",
      "23 Cost:  6.26191 \n",
      "Prediction:\n",
      " [[-0.9646238]\n",
      " [-2.045157 ]\n",
      " [-1.9933589]\n",
      " [-1.9298428]\n",
      " [-1.8836774]\n",
      " [-1.9575379]\n",
      " [-2.3901963]\n",
      " [-2.701619 ]]\n",
      "24 Cost:  6.261468 \n",
      "Prediction:\n",
      " [[-0.9645064]\n",
      " [-2.045039 ]\n",
      " [-1.9932599]\n",
      " [-1.9297655]\n",
      " [-1.8835866]\n",
      " [-1.9574499]\n",
      " [-2.3901343]\n",
      " [-2.7015567]]\n",
      "25 Cost:  6.2610254 \n",
      "Prediction:\n",
      " [[-0.96438885]\n",
      " [-2.0449207 ]\n",
      " [-1.993161  ]\n",
      " [-1.929688  ]\n",
      " [-1.883496  ]\n",
      " [-1.9573619 ]\n",
      " [-2.3900723 ]\n",
      " [-2.7014945 ]]\n",
      "26 Cost:  6.260583 \n",
      "Prediction:\n",
      " [[-0.9642714]\n",
      " [-2.0448027]\n",
      " [-1.9930619]\n",
      " [-1.9296106]\n",
      " [-1.8834053]\n",
      " [-1.9572741]\n",
      " [-2.3900104]\n",
      " [-2.7014322]]\n",
      "27 Cost:  6.26014 \n",
      "Prediction:\n",
      " [[-0.9641539]\n",
      " [-2.0446844]\n",
      " [-1.992963 ]\n",
      " [-1.9295332]\n",
      " [-1.8833146]\n",
      " [-1.957186 ]\n",
      " [-2.3899484]\n",
      " [-2.70137  ]]\n",
      "28 Cost:  6.259698 \n",
      "Prediction:\n",
      " [[-0.96403646]\n",
      " [-2.0445664 ]\n",
      " [-1.9928639 ]\n",
      " [-1.9294558 ]\n",
      " [-1.8832239 ]\n",
      " [-1.9570981 ]\n",
      " [-2.3898864 ]\n",
      " [-2.7013078 ]]\n",
      "29 Cost:  6.2592554 \n",
      "Prediction:\n",
      " [[-0.9639189]\n",
      " [-2.0444481]\n",
      " [-1.992765 ]\n",
      " [-1.9293783]\n",
      " [-1.8831332]\n",
      " [-1.9570103]\n",
      " [-2.3898244]\n",
      " [-2.7012455]]\n",
      "30 Cost:  6.258813 \n",
      "Prediction:\n",
      " [[-0.9638015]\n",
      " [-2.0443301]\n",
      " [-1.992666 ]\n",
      " [-1.929301 ]\n",
      " [-1.8830426]\n",
      " [-1.9569223]\n",
      " [-2.3897624]\n",
      " [-2.7011833]]\n",
      "31 Cost:  6.2583704 \n",
      "Prediction:\n",
      " [[-0.96368396]\n",
      " [-2.0442119 ]\n",
      " [-1.9925671 ]\n",
      " [-1.9292235 ]\n",
      " [-1.8829519 ]\n",
      " [-1.9568343 ]\n",
      " [-2.3897004 ]\n",
      " [-2.701121  ]]\n",
      "32 Cost:  6.257928 \n",
      "Prediction:\n",
      " [[-0.96356654]\n",
      " [-2.0440938 ]\n",
      " [-1.992468  ]\n",
      " [-1.929146  ]\n",
      " [-1.8828611 ]\n",
      " [-1.9567463 ]\n",
      " [-2.3896384 ]\n",
      " [-2.7010589 ]]\n",
      "33 Cost:  6.2574854 \n",
      "Prediction:\n",
      " [[-0.963449 ]\n",
      " [-2.0439758]\n",
      " [-1.992369 ]\n",
      " [-1.9290688]\n",
      " [-1.8827704]\n",
      " [-1.9566584]\n",
      " [-2.3895764]\n",
      " [-2.7009966]]\n",
      "34 Cost:  6.2570434 \n",
      "Prediction:\n",
      " [[-0.9633316]\n",
      " [-2.0438576]\n",
      " [-1.9922701]\n",
      " [-1.9289913]\n",
      " [-1.8826798]\n",
      " [-1.9565705]\n",
      " [-2.3895144]\n",
      " [-2.7009344]]\n",
      "35 Cost:  6.2566013 \n",
      "Prediction:\n",
      " [[-0.96321416]\n",
      " [-2.0437396 ]\n",
      " [-1.992171  ]\n",
      " [-1.928914  ]\n",
      " [-1.8825891 ]\n",
      " [-1.9564825 ]\n",
      " [-2.3894525 ]\n",
      " [-2.7008722 ]]\n",
      "36 Cost:  6.256159 \n",
      "Prediction:\n",
      " [[-0.9630966]\n",
      " [-2.0436213]\n",
      " [-1.9920721]\n",
      " [-1.9288366]\n",
      " [-1.8824984]\n",
      " [-1.9563946]\n",
      " [-2.3893905]\n",
      " [-2.70081  ]]\n",
      "37 Cost:  6.2557163 \n",
      "Prediction:\n",
      " [[-0.9629792]\n",
      " [-2.0435033]\n",
      " [-1.9919732]\n",
      " [-1.9287591]\n",
      " [-1.8824077]\n",
      " [-1.9563067]\n",
      " [-2.3893285]\n",
      " [-2.7007477]]\n",
      "38 Cost:  6.255274 \n",
      "Prediction:\n",
      " [[-0.96286166]\n",
      " [-2.043385  ]\n",
      " [-1.9918741 ]\n",
      " [-1.9286817 ]\n",
      " [-1.882317  ]\n",
      " [-1.9562187 ]\n",
      " [-2.3892665 ]\n",
      " [-2.7006855 ]]\n",
      "39 Cost:  6.2548323 \n",
      "Prediction:\n",
      " [[-0.96274424]\n",
      " [-2.043267  ]\n",
      " [-1.991775  ]\n",
      " [-1.9286044 ]\n",
      " [-1.8822263 ]\n",
      " [-1.9561307 ]\n",
      " [-2.3892045 ]\n",
      " [-2.7006233 ]]\n",
      "40 Cost:  6.25439 \n",
      "Prediction:\n",
      " [[-0.9626267]\n",
      " [-2.0431488]\n",
      " [-1.9916761]\n",
      " [-1.9285269]\n",
      " [-1.8821356]\n",
      " [-1.9560429]\n",
      " [-2.3891428]\n",
      " [-2.700561 ]]\n",
      "41 Cost:  6.2539473 \n",
      "Prediction:\n",
      " [[-0.9625093]\n",
      " [-2.0430307]\n",
      " [-1.9915771]\n",
      " [-1.9284495]\n",
      " [-1.882045 ]\n",
      " [-1.9559549]\n",
      " [-2.3890808]\n",
      " [-2.7004988]]\n",
      "42 Cost:  6.2535057 \n",
      "Prediction:\n",
      " [[-0.96239185]\n",
      " [-2.0429125 ]\n",
      " [-1.9914782 ]\n",
      " [-1.9283721 ]\n",
      " [-1.8819543 ]\n",
      " [-1.955867  ]\n",
      " [-2.3890188 ]\n",
      " [-2.7004366 ]]\n",
      "43 Cost:  6.253063 \n",
      "Prediction:\n",
      " [[-0.9622743]\n",
      " [-2.0427945]\n",
      " [-1.9913793]\n",
      " [-1.9282947]\n",
      " [-1.8818636]\n",
      " [-1.9557791]\n",
      " [-2.3889568]\n",
      " [-2.7003744]]\n",
      "44 Cost:  6.252621 \n",
      "Prediction:\n",
      " [[-0.9621569]\n",
      " [-2.0426764]\n",
      " [-1.9912803]\n",
      " [-1.9282173]\n",
      " [-1.8817729]\n",
      " [-1.9556911]\n",
      " [-2.3888948]\n",
      " [-2.7003121]]\n",
      "45 Cost:  6.252179 \n",
      "Prediction:\n",
      " [[-0.9620395]\n",
      " [-2.0425582]\n",
      " [-1.9911813]\n",
      " [-1.9281399]\n",
      " [-1.8816822]\n",
      " [-1.9556031]\n",
      " [-2.3888328]\n",
      " [-2.70025  ]]\n",
      "46 Cost:  6.251737 \n",
      "Prediction:\n",
      " [[-0.96192193]\n",
      " [-2.0424402 ]\n",
      " [-1.9910823 ]\n",
      " [-1.9280626 ]\n",
      " [-1.8815914 ]\n",
      " [-1.9555151 ]\n",
      " [-2.3887708 ]\n",
      " [-2.7001877 ]]\n",
      "47 Cost:  6.251295 \n",
      "Prediction:\n",
      " [[-0.9618045]\n",
      " [-2.0423222]\n",
      " [-1.9909832]\n",
      " [-1.9279851]\n",
      " [-1.8815008]\n",
      " [-1.9554273]\n",
      " [-2.3887088]\n",
      " [-2.7001255]]\n",
      "48 Cost:  6.2508526 \n",
      "Prediction:\n",
      " [[-0.9616871]\n",
      " [-2.042204 ]\n",
      " [-1.9908843]\n",
      " [-1.9279077]\n",
      " [-1.8814101]\n",
      " [-1.9553393]\n",
      " [-2.3886468]\n",
      " [-2.7000632]]\n",
      "49 Cost:  6.250411 \n",
      "Prediction:\n",
      " [[-0.96156955]\n",
      " [-2.042086  ]\n",
      " [-1.9907854 ]\n",
      " [-1.9278303 ]\n",
      " [-1.8813195 ]\n",
      " [-1.9552513 ]\n",
      " [-2.3885849 ]\n",
      " [-2.700001  ]]\n",
      "50 Cost:  6.2499685 \n",
      "Prediction:\n",
      " [[-0.9614521]\n",
      " [-2.0419679]\n",
      " [-1.9906864]\n",
      " [-1.9277529]\n",
      " [-1.8812288]\n",
      " [-1.9551635]\n",
      " [-2.3885229]\n",
      " [-2.6999388]]\n",
      "51 Cost:  6.2495265 \n",
      "Prediction:\n",
      " [[-0.9613347]\n",
      " [-2.0418496]\n",
      " [-1.9905875]\n",
      " [-1.9276755]\n",
      " [-1.8811381]\n",
      " [-1.9550755]\n",
      " [-2.3884609]\n",
      " [-2.6998765]]\n",
      "52 Cost:  6.2490854 \n",
      "Prediction:\n",
      " [[-0.96121716]\n",
      " [-2.0417316 ]\n",
      " [-1.9904884 ]\n",
      " [-1.9275981 ]\n",
      " [-1.8810475 ]\n",
      " [-1.9549875 ]\n",
      " [-2.388399  ]\n",
      " [-2.6998143 ]]\n",
      "53 Cost:  6.248643 \n",
      "Prediction:\n",
      " [[-0.96109974]\n",
      " [-2.0416133 ]\n",
      " [-1.9903895 ]\n",
      " [-1.9275208 ]\n",
      " [-1.8809568 ]\n",
      " [-1.9548997 ]\n",
      " [-2.388337  ]\n",
      " [-2.699752  ]]\n",
      "54 Cost:  6.248201 \n",
      "Prediction:\n",
      " [[-0.9609823]\n",
      " [-2.0414953]\n",
      " [-1.9902904]\n",
      " [-1.9274433]\n",
      " [-1.880866 ]\n",
      " [-1.9548117]\n",
      " [-2.388275 ]\n",
      " [-2.6996899]]\n",
      "55 Cost:  6.247759 \n",
      "Prediction:\n",
      " [[-0.9608648]\n",
      " [-2.041377 ]\n",
      " [-1.9901915]\n",
      " [-1.9273659]\n",
      " [-1.8807755]\n",
      " [-1.9547238]\n",
      " [-2.3882132]\n",
      " [-2.6996276]]\n",
      "56 Cost:  6.2473173 \n",
      "Prediction:\n",
      " [[-0.96074736]\n",
      " [-2.041259  ]\n",
      " [-1.9900925 ]\n",
      " [-1.9272885 ]\n",
      " [-1.8806846 ]\n",
      " [-1.9546359 ]\n",
      " [-2.3881512 ]\n",
      " [-2.6995654 ]]\n",
      "57 Cost:  6.2468767 \n",
      "Prediction:\n",
      " [[-0.9606302]\n",
      " [-2.0411413]\n",
      " [-1.9899938]\n",
      " [-1.9272113]\n",
      " [-1.8805943]\n",
      " [-1.9545481]\n",
      " [-2.3880894]\n",
      " [-2.6995034]]\n",
      "58 Cost:  6.246435 \n",
      "Prediction:\n",
      " [[-0.960513 ]\n",
      " [-2.0410233]\n",
      " [-1.9898951]\n",
      " [-1.9271342]\n",
      " [-1.8805039]\n",
      " [-1.9544605]\n",
      " [-2.3880277]\n",
      " [-2.6994414]]\n",
      "59 Cost:  6.245995 \n",
      "Prediction:\n",
      " [[-0.9603957]\n",
      " [-2.0409055]\n",
      " [-1.9897964]\n",
      " [-1.927057 ]\n",
      " [-1.8804134]\n",
      " [-1.9543729]\n",
      " [-2.387966 ]\n",
      " [-2.6993794]]\n",
      "60 Cost:  6.245554 \n",
      "Prediction:\n",
      " [[-0.9602785]\n",
      " [-2.0407877]\n",
      " [-1.9896976]\n",
      " [-1.9269799]\n",
      " [-1.8803229]\n",
      " [-1.9542851]\n",
      " [-2.3879042]\n",
      " [-2.6993175]]\n",
      "61 Cost:  6.245114 \n",
      "Prediction:\n",
      " [[-0.9601613]\n",
      " [-2.0406697]\n",
      " [-1.9895989]\n",
      " [-1.9269028]\n",
      " [-1.8802325]\n",
      " [-1.9541974]\n",
      " [-2.3878424]\n",
      " [-2.6992555]]\n",
      "62 Cost:  6.2446733 \n",
      "Prediction:\n",
      " [[-0.96004415]\n",
      " [-2.040552  ]\n",
      " [-1.9895    ]\n",
      " [-1.9268255 ]\n",
      " [-1.880142  ]\n",
      " [-1.9541097 ]\n",
      " [-2.3877807 ]\n",
      " [-2.6991935 ]]\n",
      "63 Cost:  6.244232 \n",
      "Prediction:\n",
      " [[-0.95992684]\n",
      " [-2.0404341 ]\n",
      " [-1.9894015 ]\n",
      " [-1.9267484 ]\n",
      " [-1.8800516 ]\n",
      " [-1.954022  ]\n",
      " [-2.387719  ]\n",
      " [-2.6991315 ]]\n",
      "64 Cost:  6.2437916 \n",
      "Prediction:\n",
      " [[-0.95980966]\n",
      " [-2.040316  ]\n",
      " [-1.9893026 ]\n",
      " [-1.9266713 ]\n",
      " [-1.8799613 ]\n",
      " [-1.9539343 ]\n",
      " [-2.3876572 ]\n",
      " [-2.6990695 ]]\n",
      "65 Cost:  6.243351 \n",
      "Prediction:\n",
      " [[-0.9596925]\n",
      " [-2.0401983]\n",
      " [-1.9892039]\n",
      " [-1.926594 ]\n",
      " [-1.8798708]\n",
      " [-1.9538466]\n",
      " [-2.3875954]\n",
      " [-2.6990075]]\n",
      "66 Cost:  6.2429104 \n",
      "Prediction:\n",
      " [[-0.9595753]\n",
      " [-2.0400805]\n",
      " [-1.9891052]\n",
      " [-1.9265169]\n",
      " [-1.8797803]\n",
      " [-1.953759 ]\n",
      " [-2.3875337]\n",
      " [-2.6989455]]\n",
      "67 Cost:  6.2424703 \n",
      "Prediction:\n",
      " [[-0.9594581]\n",
      " [-2.0399625]\n",
      " [-1.9890065]\n",
      " [-1.9264398]\n",
      " [-1.8796899]\n",
      " [-1.9536712]\n",
      " [-2.387472 ]\n",
      " [-2.6988835]]\n",
      "68 Cost:  6.2420297 \n",
      "Prediction:\n",
      " [[-0.95934093]\n",
      " [-2.0398448 ]\n",
      " [-1.9889078 ]\n",
      " [-1.9263625 ]\n",
      " [-1.8795995 ]\n",
      " [-1.9535835 ]\n",
      " [-2.3874102 ]\n",
      " [-2.6988215 ]]\n",
      "69 Cost:  6.241589 \n",
      "Prediction:\n",
      " [[-0.9592236]\n",
      " [-2.0397267]\n",
      " [-1.9888091]\n",
      " [-1.9262855]\n",
      " [-1.879509 ]\n",
      " [-1.9534959]\n",
      " [-2.3873487]\n",
      " [-2.6987596]]\n",
      "70 Cost:  6.241149 \n",
      "Prediction:\n",
      " [[-0.95910645]\n",
      " [-2.039609  ]\n",
      " [-1.9887103 ]\n",
      " [-1.9262083 ]\n",
      " [-1.8794186 ]\n",
      " [-1.9534082 ]\n",
      " [-2.387287  ]\n",
      " [-2.6986976 ]]\n",
      "71 Cost:  6.240708 \n",
      "Prediction:\n",
      " [[-0.95898926]\n",
      " [-2.0394912 ]\n",
      " [-1.9886116 ]\n",
      " [-1.9261311 ]\n",
      " [-1.8793281 ]\n",
      " [-1.9533205 ]\n",
      " [-2.3872252 ]\n",
      " [-2.6986356 ]]\n",
      "72 Cost:  6.2402678 \n",
      "Prediction:\n",
      " [[-0.9588721]\n",
      " [-2.0393734]\n",
      " [-1.9885129]\n",
      " [-1.926054 ]\n",
      " [-1.8792377]\n",
      " [-1.9532328]\n",
      " [-2.3871634]\n",
      " [-2.6985736]]\n",
      "73 Cost:  6.239827 \n",
      "Prediction:\n",
      " [[-0.9587549]\n",
      " [-2.0392554]\n",
      " [-1.9884142]\n",
      " [-1.9259768]\n",
      " [-1.8791473]\n",
      " [-1.953145 ]\n",
      " [-2.3871017]\n",
      " [-2.6985116]]\n",
      "74 Cost:  6.2393875 \n",
      "Prediction:\n",
      " [[-0.9586377]\n",
      " [-2.0391376]\n",
      " [-1.9883153]\n",
      " [-1.9258997]\n",
      " [-1.8790568]\n",
      " [-1.9530574]\n",
      " [-2.38704  ]\n",
      " [-2.6984496]]\n",
      "75 Cost:  6.2389464 \n",
      "Prediction:\n",
      " [[-0.95852053]\n",
      " [-2.0390196 ]\n",
      " [-1.9882166 ]\n",
      " [-1.9258225 ]\n",
      " [-1.8789665 ]\n",
      " [-1.9529698 ]\n",
      " [-2.3869781 ]\n",
      " [-2.6983876 ]]\n",
      "76 Cost:  6.2385063 \n",
      "Prediction:\n",
      " [[-0.9584032]\n",
      " [-2.0389018]\n",
      " [-1.9881179]\n",
      " [-1.9257454]\n",
      " [-1.878876 ]\n",
      " [-1.952882 ]\n",
      " [-2.3869164]\n",
      " [-2.6983256]]\n",
      "77 Cost:  6.2380657 \n",
      "Prediction:\n",
      " [[-0.95828605]\n",
      " [-2.038784  ]\n",
      " [-1.9880192 ]\n",
      " [-1.9256682 ]\n",
      " [-1.8787856 ]\n",
      " [-1.9527943 ]\n",
      " [-2.3868546 ]\n",
      " [-2.6982636 ]]\n",
      "78 Cost:  6.2376256 \n",
      "Prediction:\n",
      " [[-0.95816886]\n",
      " [-2.0386662 ]\n",
      " [-1.9879205 ]\n",
      " [-1.925591  ]\n",
      " [-1.8786951 ]\n",
      " [-1.9527067 ]\n",
      " [-2.386793  ]\n",
      " [-2.6982017 ]]\n",
      "79 Cost:  6.2371855 \n",
      "Prediction:\n",
      " [[-0.9580517]\n",
      " [-2.0385485]\n",
      " [-1.9878218]\n",
      " [-1.925514 ]\n",
      " [-1.8786048]\n",
      " [-1.9526191]\n",
      " [-2.3867311]\n",
      " [-2.6981397]]\n",
      "80 Cost:  6.2367454 \n",
      "Prediction:\n",
      " [[-0.9579345]\n",
      " [-2.0384305]\n",
      " [-1.9877231]\n",
      " [-1.9254369]\n",
      " [-1.8785143]\n",
      " [-1.9525313]\n",
      " [-2.3866696]\n",
      " [-2.6980777]]\n",
      "81 Cost:  6.2363057 \n",
      "Prediction:\n",
      " [[-0.95781744]\n",
      " [-2.0383127 ]\n",
      " [-1.9876244 ]\n",
      " [-1.9253597 ]\n",
      " [-1.8784239 ]\n",
      " [-1.9524437 ]\n",
      " [-2.386608  ]\n",
      " [-2.6980157 ]]\n",
      "82 Cost:  6.235865 \n",
      "Prediction:\n",
      " [[-0.9577004]\n",
      " [-2.038195 ]\n",
      " [-1.9875257]\n",
      " [-1.9252825]\n",
      " [-1.8783336]\n",
      " [-1.9523561]\n",
      " [-2.3865461]\n",
      " [-2.6979537]]\n",
      "83 Cost:  6.235425 \n",
      "Prediction:\n",
      " [[-0.9575832]\n",
      " [-2.038077 ]\n",
      " [-1.987427 ]\n",
      " [-1.9252055]\n",
      " [-1.8782431]\n",
      " [-1.9522684]\n",
      " [-2.3864844]\n",
      " [-2.6978915]]\n",
      "84 Cost:  6.234985 \n",
      "Prediction:\n",
      " [[-0.957466 ]\n",
      " [-2.037959 ]\n",
      " [-1.9873284]\n",
      " [-1.9251283]\n",
      " [-1.8781528]\n",
      " [-1.9521807]\n",
      " [-2.3864226]\n",
      " [-2.6978295]]\n",
      "85 Cost:  6.2345448 \n",
      "Prediction:\n",
      " [[-0.95734894]\n",
      " [-2.0378413 ]\n",
      " [-1.9872297 ]\n",
      " [-1.9250512 ]\n",
      " [-1.8780624 ]\n",
      " [-1.9520931 ]\n",
      " [-2.386361  ]\n",
      " [-2.6977675 ]]\n",
      "86 Cost:  6.234104 \n",
      "Prediction:\n",
      " [[-0.9572319]\n",
      " [-2.0377235]\n",
      " [-1.9871311]\n",
      " [-1.9249741]\n",
      " [-1.8779719]\n",
      " [-1.9520054]\n",
      " [-2.3862991]\n",
      " [-2.6977055]]\n",
      "87 Cost:  6.2336645 \n",
      "Prediction:\n",
      " [[-0.9571147]\n",
      " [-2.0376058]\n",
      " [-1.9870323]\n",
      " [-1.924897 ]\n",
      " [-1.8778815]\n",
      " [-1.9519179]\n",
      " [-2.3862376]\n",
      " [-2.6976435]]\n",
      "88 Cost:  6.2332244 \n",
      "Prediction:\n",
      " [[-0.9569975]\n",
      " [-2.037488 ]\n",
      " [-1.9869337]\n",
      " [-1.9248198]\n",
      " [-1.8777912]\n",
      " [-1.9518301]\n",
      " [-2.3861759]\n",
      " [-2.6975815]]\n",
      "89 Cost:  6.2327847 \n",
      "Prediction:\n",
      " [[-0.95688045]\n",
      " [-2.0373702 ]\n",
      " [-1.9868349 ]\n",
      " [-1.9247427 ]\n",
      " [-1.8777008 ]\n",
      " [-1.9517425 ]\n",
      " [-2.3861141 ]\n",
      " [-2.6975195 ]]\n",
      "90 Cost:  6.2323446 \n",
      "Prediction:\n",
      " [[-0.9567634]\n",
      " [-2.0372524]\n",
      " [-1.9867363]\n",
      " [-1.9246656]\n",
      " [-1.8776104]\n",
      " [-1.9516549]\n",
      " [-2.3860524]\n",
      " [-2.6974576]]\n",
      "91 Cost:  6.231905 \n",
      "Prediction:\n",
      " [[-0.9566462]\n",
      " [-2.0371346]\n",
      " [-1.9866376]\n",
      " [-1.9245884]\n",
      " [-1.8775201]\n",
      " [-1.9515672]\n",
      " [-2.3859906]\n",
      " [-2.6973956]]\n",
      "92 Cost:  6.2314653 \n",
      "Prediction:\n",
      " [[-0.95652926]\n",
      " [-2.0370169 ]\n",
      " [-1.986539  ]\n",
      " [-1.9245114 ]\n",
      " [-1.8774297 ]\n",
      " [-1.9514797 ]\n",
      " [-2.3859289 ]\n",
      " [-2.6973336 ]]\n",
      "93 Cost:  6.231025 \n",
      "Prediction:\n",
      " [[-0.9564121]\n",
      " [-2.036899 ]\n",
      " [-1.9864404]\n",
      " [-1.9244342]\n",
      " [-1.8773394]\n",
      " [-1.9513919]\n",
      " [-2.385867 ]\n",
      " [-2.6972716]]\n",
      "94 Cost:  6.230586 \n",
      "Prediction:\n",
      " [[-0.95629513]\n",
      " [-2.0367813 ]\n",
      " [-1.9863417 ]\n",
      " [-1.9243572 ]\n",
      " [-1.877249  ]\n",
      " [-1.9513044 ]\n",
      " [-2.3858054 ]\n",
      " [-2.6972096 ]]\n",
      "95 Cost:  6.2301455 \n",
      "Prediction:\n",
      " [[-0.95617795]\n",
      " [-2.0366635 ]\n",
      " [-1.986243  ]\n",
      " [-1.92428   ]\n",
      " [-1.8771585 ]\n",
      " [-1.9512167 ]\n",
      " [-2.3857439 ]\n",
      " [-2.6971476 ]]\n",
      "96 Cost:  6.229706 \n",
      "Prediction:\n",
      " [[-0.956061 ]\n",
      " [-2.0365458]\n",
      " [-1.9861443]\n",
      " [-1.9242029]\n",
      " [-1.8770682]\n",
      " [-1.9511291]\n",
      " [-2.385682 ]\n",
      " [-2.6970856]]\n",
      "97 Cost:  6.229266 \n",
      "Prediction:\n",
      " [[-0.9559438]\n",
      " [-2.036428 ]\n",
      " [-1.9860457]\n",
      " [-1.9241259]\n",
      " [-1.8769779]\n",
      " [-1.9510415]\n",
      " [-2.3856204]\n",
      " [-2.6970236]]\n",
      "98 Cost:  6.228827 \n",
      "Prediction:\n",
      " [[-0.9558269]\n",
      " [-2.0363102]\n",
      " [-1.9859471]\n",
      " [-1.9240488]\n",
      " [-1.8768876]\n",
      " [-1.950954 ]\n",
      " [-2.3855586]\n",
      " [-2.6969619]]\n",
      "99 Cost:  6.2283874 \n",
      "Prediction:\n",
      " [[-0.9557097]\n",
      " [-2.0361927]\n",
      " [-1.9858484]\n",
      " [-1.9239717]\n",
      " [-1.8767972]\n",
      " [-1.9508662]\n",
      " [-2.3854969]\n",
      " [-2.6969   ]]\n",
      "100 Cost:  6.227948 \n",
      "Prediction:\n",
      " [[-0.95559275]\n",
      " [-2.036075  ]\n",
      " [-1.98575   ]\n",
      " [-1.9238945 ]\n",
      " [-1.8767068 ]\n",
      " [-1.9507787 ]\n",
      " [-2.385435  ]\n",
      " [-2.696838  ]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "xy = np.array([[828.659973, 833.450012, 908100, 828.349976, 831.659973],\n",
    "               [823.02002, 828.070007, 1828100, 821.655029, 828.070007],\n",
    "               [819.929993, 824.400024, 1438100, 818.97998, 824.159973],\n",
    "               [816, 820.958984, 1008100, 815.48999, 819.23999],\n",
    "               [819.359985, 823, 1188100, 818.469971, 818.97998],\n",
    "               [819, 823, 1198100, 816, 820.450012],\n",
    "               [811.700012, 815.25, 1098100, 809.780029, 813.669983],\n",
    "               [809.51001, 816.659973, 1398100, 804.539978, 809.559998]])\n",
    "\n",
    "x_data = xy[:, 0:-1]\n",
    "y_data = xy[:, [-1]]\n",
    "\n",
    "W = tf.Variable(tf.random.normal([4, 1]), name='weight')\n",
    "b = tf.Variable(tf.random.normal([1]), name='bias')\n",
    "\n",
    "def model(X):\n",
    "    return tf.matmul(X, W) + b\n",
    "\n",
    "def loss(Y, hypothesis):\n",
    "    return tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "\n",
    "optimizer = tf.optimizers.SGD(learning_rate=1e-5)\n",
    "\n",
    "def train(X, Y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        hypothesis = model(X)\n",
    "        cost = loss(Y, hypothesis)\n",
    "    gradients = tape.gradient(cost, [W, b])\n",
    "    optimizer.apply_gradients(zip(gradients, [W, b]))\n",
    "\n",
    "# Min-Max Normalization\n",
    "def min_max_normalize(dataset):\n",
    "    min_value = np.min(dataset, axis=0)\n",
    "    max_value = np.max(dataset, axis=0)\n",
    "    \n",
    "    return (dataset - min_value) / (max_value - min_value)\n",
    "\n",
    "x_data_normalized = min_max_normalize(x_data)\n",
    "y_data_normalized = min_max_normalize(y_data)\n",
    "\n",
    "X = tf.constant(x_data_normalized, dtype=tf.float32)\n",
    "Y = tf.constant(y_data_normalized, dtype=tf.float32)\n",
    "\n",
    "for step in range(101):\n",
    "    train(X, Y)\n",
    "    cost_val = loss(Y, model(X))\n",
    "    hy_val = model(X)\n",
    "    print(step, \"Cost: \", cost_val.numpy(), \"\\nPrediction:\\n\", hy_val.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "DLL load failed while importing ft2font: 메모리 리소스가 부족하여 이 명령을 처리할 수 없습니다.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-b3f4e855ea88>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmnist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmnist\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\matplotlib\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    172\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 174\u001b[1;33m \u001b[0m_check_versions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    175\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    176\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\matplotlib\\__init__.py\u001b[0m in \u001b[0;36m_check_versions\u001b[1;34m()\u001b[0m\n\u001b[0;32m    157\u001b[0m     \u001b[1;31m# Quickfix to ensure Microsoft Visual C++ redistributable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m     \u001b[1;31m# DLLs are loaded before importing kiwisolver\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 159\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mft2font\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    160\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m     for modname, minver in [\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed while importing ft2font: 메모리 리소스가 부족하여 이 명령을 처리할 수 없습니다."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "# 데이터셋을 로드하고 훈련 및 테스트 세트로 분할\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# 정규화\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "# 원-핫 인코딩\n",
    "y_train = tf.one_hot(y_train, depth=10)\n",
    "y_test = tf.one_hot(y_test, depth=10)\n",
    "\n",
    "# MNIST 이미지는 28x28 크기이므로 784 길이의 벡터로 변환\n",
    "x_train = x_train.reshape(-1, 28*28)\n",
    "x_test = x_test.reshape(-1, 28*28)\n",
    "\n",
    "# 변수 설정 (Weight 초기화 방법 변경)\n",
    "W = tf.Variable(tf.random.uniform([784, 10], minval=-1., maxval=1.))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "# 가설 설정 (소프트맥스 사용)\n",
    "@tf.function\n",
    "def model(X):\n",
    "    return tf.nn.softmax(tf.matmul(X, W) + b)\n",
    "\n",
    "# 비용함수 (크로스 엔트로피 사용)\n",
    "def cost(X, Y):\n",
    "    return -tf.reduce_mean(tf.reduce_sum(Y * tf.math.log(model(X)), axis=1))\n",
    "\n",
    "# 경사 하강법 사용한 최적화\n",
    "def train(X, Y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        c = cost(X, Y)\n",
    "        \n",
    "    gradients = tape.gradient(c, [W, b])\n",
    "    optimizer = tf.optimizers.SGD(learning_rate=0.1)\n",
    "    optimizer.apply_gradients(zip(gradients, [W, b]))\n",
    "\n",
    "# 모델 테스트\n",
    "def is_correct(X, Y):\n",
    "    return tf.equal(tf.argmax(model(X), 1), tf.argmax(Y, 1))\n",
    "\n",
    "# 정확도 계산\n",
    "def accuracy(X, Y):\n",
    "    return tf.reduce_mean(tf.cast(is_correct(X, Y), tf.float32))\n",
    "\n",
    "# 매개변수\n",
    "num_epochs = 15\n",
    "batch_size = 100\n",
    "num_iterations = x_train.shape[0] // batch_size\n",
    "\n",
    "# 훈련 사이클\n",
    "for epoch in range(num_epochs):\n",
    "    avg_cost = 0\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        batch_xs, batch_ys = x_train[i * batch_size:(i+1) * batch_size], y_train[i * batch_size:(i+1) * batch_size]\n",
    "        train(batch_xs, batch_ys)\n",
    "        avg_cost += cost(batch_xs, batch_ys) / num_iterations\n",
    "\n",
    "    print(\"Epoch: {:04d}, Cost: {:.9f}\".format(epoch + 1, avg_cost))\n",
    "\n",
    "print(\"Learning finished\")\n",
    "\n",
    "# 테스트 셋을 사용하여 모델 테스트 및 정확도 계산\n",
    "print(\"Accuracy: \", accuracy(x_test, y_test).numpy())\n",
    "\n",
    "# 하나의 예측값 가져오기\n",
    "r = random.randint(0, x_test.shape[0] - 1)\n",
    "print(\"Label: \", tf.argmax(y_test[r : r + 1], 1).numpy())\n",
    "print(\"Prediction: \", tf.argmax(model(x_test[r : r + 1]), 1).numpy())\n",
    "\n",
    "# 예측한 이미지 출력\n",
    "plt.imshow(x_test[r : r + 1].reshape(28, 28), cmap=\"Greys\", interpolation=\"nearest\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda update -n base -c defaults conda"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
