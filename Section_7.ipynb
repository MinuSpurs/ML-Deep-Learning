{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "1 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "2 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "3 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "4 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "5 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "6 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "7 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "8 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "9 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "10 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "11 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "12 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "13 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "14 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "15 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "16 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "17 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "18 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "19 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "20 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "21 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "22 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "23 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "24 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "25 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "26 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "27 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "28 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "29 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "30 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "31 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "32 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "33 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "34 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "35 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "36 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "37 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "38 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "39 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "40 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "41 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "42 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "43 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "44 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "45 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "46 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "47 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "48 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "49 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "50 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "51 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "52 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "53 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "54 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "55 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "56 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "57 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "58 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "59 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "60 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "61 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "62 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "63 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "64 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "65 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "66 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "67 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "68 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "69 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "70 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "71 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "72 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "73 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "74 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "75 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "76 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "77 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "78 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "79 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "80 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "81 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "82 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "83 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "84 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "85 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "86 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "87 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "88 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "89 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "90 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "91 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "92 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "93 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "94 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "95 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "96 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "97 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "98 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "99 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "100 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "101 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "102 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "103 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "104 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "105 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "106 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "107 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "108 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "109 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "110 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "111 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "112 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "113 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "114 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "115 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "116 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "117 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "118 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "119 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "120 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "121 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "122 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "123 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "124 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "125 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "126 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "127 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "128 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "129 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "130 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "131 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "132 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "133 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "134 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "135 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "136 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "137 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "138 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "139 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "140 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "141 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "142 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "143 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "144 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "145 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "146 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "147 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "148 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "149 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "150 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "151 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "152 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "153 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "154 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "155 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "156 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "157 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "158 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "159 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "160 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "161 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "162 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "163 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "164 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "165 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "166 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "167 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "168 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "169 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "170 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "171 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "172 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "173 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "174 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "175 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "176 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "177 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "178 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "179 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "180 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "181 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "182 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "183 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "184 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "185 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "186 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "187 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "188 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "189 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "190 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "191 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "192 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "193 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "194 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "195 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "196 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "197 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "198 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "199 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "200 -2.8777475 [[ 0.6180538  -0.64044863 -0.30305913]\n",
      " [ 0.8063584   0.09972315  0.68362045]\n",
      " [-0.4138558  -0.9952862  -0.87643915]]\n",
      "Prediction: [0 0 0]\n",
      "Accuracy:  0.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "x_data = [[1, 2, 1],\n",
    "          [1, 3, 2],\n",
    "          [1, 3, 4],\n",
    "          [1, 5, 5],\n",
    "          [1, 7, 5],\n",
    "          [1, 2, 5],\n",
    "          [1, 6, 6],\n",
    "          [1, 7, 7]]\n",
    "y_data = [[0, 0, 1],\n",
    "          [0, 0, 1],\n",
    "          [0, 0, 1],\n",
    "          [0, 1, 0],\n",
    "          [0, 1, 0],\n",
    "          [0, 1, 0],\n",
    "          [1, 0, 0],\n",
    "          [1, 0, 0]]\n",
    "\n",
    "x_test = [[2, 1, 1],\n",
    "          [3, 1, 2],\n",
    "          [3, 3, 4]]\n",
    "y_test = [[0, 0, 1],\n",
    "          [0, 0, 1],\n",
    "          [0, 0, 1]]\n",
    "\n",
    "X = tf.constant(x_data, dtype=tf.float32)\n",
    "Y = tf.constant(y_data, dtype=tf.float32)\n",
    "\n",
    "W = tf.Variable(tf.random.normal([3, 3]))\n",
    "b = tf.Variable(tf.random.normal([3]))\n",
    "\n",
    "#hypothesis function\n",
    "def model(X, W, b):\n",
    "    return tf.nn.softmax(tf.matmul(X, W) + b)\n",
    "\n",
    "#cross enthropy\n",
    "def loss(Y, hypothesis):\n",
    "    return tf.reduce_mean(tf.reduce_sum(Y * tf.math.log(hypothesis), axis=1))\n",
    "\n",
    "def train(X, Y, W, b, learning_rate=1e-10):\n",
    "    with tf.GradientTape() as tape:\n",
    "        hypothesis = model(X, W, b)\n",
    "        cost = loss(Y, hypothesis)\n",
    "        \n",
    "    gradients = tape.gradient(cost, [W, b])\n",
    "    \n",
    "    W.assign_sub(learning_rate * gradients[0])\n",
    "    b.assign_sub(learning_rate * gradients[1])\n",
    "\n",
    "def predict(X, W, b):\n",
    "    return tf.argmax(model(X, W, b), 1)\n",
    "\n",
    "def accuracy(X, Y, W, b):\n",
    "    prediction = predict(X, W, b)\n",
    "    is_correct = tf.equal(prediction, tf.argmax(Y, 1))\n",
    "    return tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "\n",
    "for step in range(201):\n",
    "    train(X, Y, W, b)\n",
    "    cost_val = loss(Y, model(X, W, b))\n",
    "    print(step, cost_val.numpy(), W.numpy())\n",
    "\n",
    "x_test_var = tf.constant(x_test, dtype=tf.float32)\n",
    "print(\"Prediction:\", predict(x_test_var, W, b).numpy())\n",
    "print(\"Accuracy: \", accuracy(x_test_var, tf.constant(y_test, dtype=tf.float32), W, b).numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Cost:  0.59047544 \n",
      "Prediction:\n",
      " [[ 0.5222579 ]\n",
      " [-0.6946373 ]\n",
      " [-0.33594805]\n",
      " [ 0.09718047]\n",
      " [-0.09469368]\n",
      " [-0.12896378]\n",
      " [-0.1977076 ]\n",
      " [-0.4831944 ]]\n",
      "1 Cost:  0.5904372 \n",
      "Prediction:\n",
      " [[ 0.5222927 ]\n",
      " [-0.6946016 ]\n",
      " [-0.33591878]\n",
      " [ 0.09720264]\n",
      " [-0.09466718]\n",
      " [-0.12893817]\n",
      " [-0.19769026]\n",
      " [-0.48317677]]\n",
      "2 Cost:  0.590399 \n",
      "Prediction:\n",
      " [[ 0.5223275 ]\n",
      " [-0.6945661 ]\n",
      " [-0.33588955]\n",
      " [ 0.09722479]\n",
      " [-0.09464069]\n",
      " [-0.12891248]\n",
      " [-0.19767295]\n",
      " [-0.4831591 ]]\n",
      "3 Cost:  0.59036076 \n",
      "Prediction:\n",
      " [[ 0.5223623 ]\n",
      " [-0.69453037]\n",
      " [-0.33586028]\n",
      " [ 0.09724694]\n",
      " [-0.09461423]\n",
      " [-0.12888688]\n",
      " [-0.19765565]\n",
      " [-0.48314148]]\n",
      "4 Cost:  0.5903225 \n",
      "Prediction:\n",
      " [[ 0.52239704]\n",
      " [-0.69449466]\n",
      " [-0.335831  ]\n",
      " [ 0.09726908]\n",
      " [-0.09458777]\n",
      " [-0.12886122]\n",
      " [-0.1976383 ]\n",
      " [-0.48312378]]\n",
      "5 Cost:  0.59028435 \n",
      "Prediction:\n",
      " [[ 0.52243185]\n",
      " [-0.69445914]\n",
      " [-0.33580172]\n",
      " [ 0.09729122]\n",
      " [-0.0945613 ]\n",
      " [-0.1288356 ]\n",
      " [-0.19762099]\n",
      " [-0.4831061 ]]\n",
      "6 Cost:  0.5902462 \n",
      "Prediction:\n",
      " [[ 0.52246666]\n",
      " [-0.69442356]\n",
      " [-0.33577242]\n",
      " [ 0.09731338]\n",
      " [-0.09453478]\n",
      " [-0.12880996]\n",
      " [-0.19760367]\n",
      " [-0.4830885 ]]\n",
      "7 Cost:  0.59020793 \n",
      "Prediction:\n",
      " [[ 0.5225014 ]\n",
      " [-0.69438785]\n",
      " [-0.3357432 ]\n",
      " [ 0.09733552]\n",
      " [-0.09450833]\n",
      " [-0.12878431]\n",
      " [-0.19758634]\n",
      " [-0.48307088]]\n",
      "8 Cost:  0.5901698 \n",
      "Prediction:\n",
      " [[ 0.5225362 ]\n",
      " [-0.6943523 ]\n",
      " [-0.33571386]\n",
      " [ 0.09735769]\n",
      " [-0.09448187]\n",
      " [-0.1287587 ]\n",
      " [-0.19756903]\n",
      " [-0.48305318]]\n",
      "9 Cost:  0.59013164 \n",
      "Prediction:\n",
      " [[ 0.52257097]\n",
      " [-0.6943167 ]\n",
      " [-0.3356846 ]\n",
      " [ 0.09737985]\n",
      " [-0.09445541]\n",
      " [-0.12873308]\n",
      " [-0.19755171]\n",
      " [-0.4830355 ]]\n",
      "10 Cost:  0.5900934 \n",
      "Prediction:\n",
      " [[ 0.5226058 ]\n",
      " [-0.6942811 ]\n",
      " [-0.33565533]\n",
      " [ 0.09740198]\n",
      " [-0.09442891]\n",
      " [-0.12870741]\n",
      " [-0.19753438]\n",
      " [-0.48301783]]\n",
      "11 Cost:  0.5900552 \n",
      "Prediction:\n",
      " [[ 0.5226406 ]\n",
      " [-0.6942455 ]\n",
      " [-0.33562607]\n",
      " [ 0.09742416]\n",
      " [-0.09440242]\n",
      " [-0.12868181]\n",
      " [-0.1975171 ]\n",
      " [-0.48300022]]\n",
      "12 Cost:  0.5900171 \n",
      "Prediction:\n",
      " [[ 0.52267534]\n",
      " [-0.6942099 ]\n",
      " [-0.3355968 ]\n",
      " [ 0.09744631]\n",
      " [-0.09437597]\n",
      " [-0.12865615]\n",
      " [-0.19749977]\n",
      " [-0.48298255]]\n",
      "13 Cost:  0.5899789 \n",
      "Prediction:\n",
      " [[ 0.52271014]\n",
      " [-0.6941743 ]\n",
      " [-0.3355675 ]\n",
      " [ 0.09746841]\n",
      " [-0.09434949]\n",
      " [-0.12863055]\n",
      " [-0.19748245]\n",
      " [-0.48296484]]\n",
      "14 Cost:  0.58994067 \n",
      "Prediction:\n",
      " [[ 0.5227449 ]\n",
      " [-0.69413865]\n",
      " [-0.3355382 ]\n",
      " [ 0.09749059]\n",
      " [-0.09432299]\n",
      " [-0.12860493]\n",
      " [-0.19746514]\n",
      " [-0.48294723]]\n",
      "15 Cost:  0.5899025 \n",
      "Prediction:\n",
      " [[ 0.5227797 ]\n",
      " [-0.69410306]\n",
      " [-0.33550897]\n",
      " [ 0.09751271]\n",
      " [-0.09429657]\n",
      " [-0.12857929]\n",
      " [-0.1974478 ]\n",
      " [-0.48292962]]\n",
      "16 Cost:  0.5898643 \n",
      "Prediction:\n",
      " [[ 0.52281445]\n",
      " [-0.6940674 ]\n",
      " [-0.33547968]\n",
      " [ 0.09753487]\n",
      " [-0.09427008]\n",
      " [-0.12855367]\n",
      " [-0.19743049]\n",
      " [-0.48291194]]\n",
      "17 Cost:  0.5898262 \n",
      "Prediction:\n",
      " [[ 0.52284926]\n",
      " [-0.69403183]\n",
      " [-0.3354504 ]\n",
      " [ 0.09755703]\n",
      " [-0.09424363]\n",
      " [-0.12852804]\n",
      " [-0.1974132 ]\n",
      " [-0.48289427]]\n",
      "18 Cost:  0.58978796 \n",
      "Prediction:\n",
      " [[ 0.5228841 ]\n",
      " [-0.6939962 ]\n",
      " [-0.33542112]\n",
      " [ 0.09757917]\n",
      " [-0.09421715]\n",
      " [-0.12850243]\n",
      " [-0.19739588]\n",
      " [-0.4828766 ]]\n",
      "19 Cost:  0.5897498 \n",
      "Prediction:\n",
      " [[ 0.5229188 ]\n",
      " [-0.69396055]\n",
      " [-0.33539185]\n",
      " [ 0.09760132]\n",
      " [-0.09419066]\n",
      " [-0.1284768 ]\n",
      " [-0.19737858]\n",
      " [-0.482859  ]]\n",
      "20 Cost:  0.58971167 \n",
      "Prediction:\n",
      " [[ 0.5229536 ]\n",
      " [-0.693925  ]\n",
      " [-0.33536258]\n",
      " [ 0.09762348]\n",
      " [-0.0941642 ]\n",
      " [-0.12845115]\n",
      " [-0.19736126]\n",
      " [-0.48284137]]\n",
      "21 Cost:  0.5896734 \n",
      "Prediction:\n",
      " [[ 0.5229884 ]\n",
      " [-0.6938893 ]\n",
      " [-0.33533332]\n",
      " [ 0.09764562]\n",
      " [-0.09413775]\n",
      " [-0.12842555]\n",
      " [-0.19734395]\n",
      " [-0.4828237 ]]\n",
      "22 Cost:  0.58963525 \n",
      "Prediction:\n",
      " [[ 0.5230232 ]\n",
      " [-0.6938538 ]\n",
      " [-0.33530402]\n",
      " [ 0.09766773]\n",
      " [-0.0941113 ]\n",
      " [-0.12839991]\n",
      " [-0.19732663]\n",
      " [-0.48280603]]\n",
      "23 Cost:  0.58959717 \n",
      "Prediction:\n",
      " [[ 0.52305794]\n",
      " [-0.6938182 ]\n",
      " [-0.3352748 ]\n",
      " [ 0.09768988]\n",
      " [-0.09408479]\n",
      " [-0.12837428]\n",
      " [-0.19730932]\n",
      " [-0.4827884 ]]\n",
      "24 Cost:  0.58955896 \n",
      "Prediction:\n",
      " [[ 0.52309275]\n",
      " [-0.69378257]\n",
      " [-0.3352455 ]\n",
      " [ 0.09771203]\n",
      " [-0.09405833]\n",
      " [-0.12834866]\n",
      " [-0.197292  ]\n",
      " [-0.48277074]]\n",
      "25 Cost:  0.5895208 \n",
      "Prediction:\n",
      " [[ 0.5231275 ]\n",
      " [-0.693747  ]\n",
      " [-0.33521622]\n",
      " [ 0.09773417]\n",
      " [-0.09403186]\n",
      " [-0.12832303]\n",
      " [-0.19727471]\n",
      " [-0.48275307]]\n",
      "26 Cost:  0.58948267 \n",
      "Prediction:\n",
      " [[ 0.52316225]\n",
      " [-0.69371134]\n",
      " [-0.33518696]\n",
      " [ 0.09775632]\n",
      " [-0.09400541]\n",
      " [-0.12829742]\n",
      " [-0.1972574 ]\n",
      " [-0.4827354 ]]\n",
      "27 Cost:  0.5894446 \n",
      "Prediction:\n",
      " [[ 0.52319705]\n",
      " [-0.6936758 ]\n",
      " [-0.3351577 ]\n",
      " [ 0.09777846]\n",
      " [-0.09397896]\n",
      " [-0.12827176]\n",
      " [-0.19724008]\n",
      " [-0.48271784]]\n",
      "28 Cost:  0.5894064 \n",
      "Prediction:\n",
      " [[ 0.5232318 ]\n",
      " [-0.69364023]\n",
      " [-0.33512843]\n",
      " [ 0.0978006 ]\n",
      " [-0.09395248]\n",
      " [-0.12824614]\n",
      " [-0.19722277]\n",
      " [-0.48270017]]\n",
      "29 Cost:  0.5893682 \n",
      "Prediction:\n",
      " [[ 0.5232666 ]\n",
      " [-0.6936046 ]\n",
      " [-0.33509916]\n",
      " [ 0.09782274]\n",
      " [-0.09392602]\n",
      " [-0.12822056]\n",
      " [-0.19720547]\n",
      " [-0.4826825 ]]\n",
      "30 Cost:  0.58933014 \n",
      "Prediction:\n",
      " [[ 0.52330136]\n",
      " [-0.693569  ]\n",
      " [-0.33506984]\n",
      " [ 0.09784489]\n",
      " [-0.09389955]\n",
      " [-0.12819491]\n",
      " [-0.19718818]\n",
      " [-0.48266482]]\n",
      "31 Cost:  0.58929193 \n",
      "Prediction:\n",
      " [[ 0.5233362 ]\n",
      " [-0.6935334 ]\n",
      " [-0.33504063]\n",
      " [ 0.097867  ]\n",
      " [-0.09387307]\n",
      " [-0.12816928]\n",
      " [-0.19717085]\n",
      " [-0.48264724]]\n",
      "32 Cost:  0.5892538 \n",
      "Prediction:\n",
      " [[ 0.5233709 ]\n",
      " [-0.6934978 ]\n",
      " [-0.33501133]\n",
      " [ 0.09788914]\n",
      " [-0.09384663]\n",
      " [-0.12814371]\n",
      " [-0.19715357]\n",
      " [-0.48262957]]\n",
      "33 Cost:  0.5892157 \n",
      "Prediction:\n",
      " [[ 0.5234057 ]\n",
      " [-0.6934622 ]\n",
      " [-0.33498207]\n",
      " [ 0.09791129]\n",
      " [-0.09382018]\n",
      " [-0.12811808]\n",
      " [-0.19713625]\n",
      " [-0.48261195]]\n",
      "34 Cost:  0.58917755 \n",
      "Prediction:\n",
      " [[ 0.5234405 ]\n",
      " [-0.6934266 ]\n",
      " [-0.3349528 ]\n",
      " [ 0.09793342]\n",
      " [-0.09379373]\n",
      " [-0.12809247]\n",
      " [-0.19711894]\n",
      " [-0.48259428]]\n",
      "35 Cost:  0.58913934 \n",
      "Prediction:\n",
      " [[ 0.5234752 ]\n",
      " [-0.69339097]\n",
      " [-0.33492354]\n",
      " [ 0.09795555]\n",
      " [-0.09376726]\n",
      " [-0.12806685]\n",
      " [-0.19710164]\n",
      " [-0.48257667]]\n",
      "36 Cost:  0.58910125 \n",
      "Prediction:\n",
      " [[ 0.52351004]\n",
      " [-0.6933554 ]\n",
      " [-0.3348943 ]\n",
      " [ 0.09797771]\n",
      " [-0.09374078]\n",
      " [-0.12804121]\n",
      " [-0.19708434]\n",
      " [-0.48255903]]\n",
      "37 Cost:  0.5890631 \n",
      "Prediction:\n",
      " [[ 0.5235448 ]\n",
      " [-0.6933198 ]\n",
      " [-0.33486503]\n",
      " [ 0.09799984]\n",
      " [-0.09371431]\n",
      " [-0.12801562]\n",
      " [-0.19706702]\n",
      " [-0.48254135]]\n",
      "38 Cost:  0.589025 \n",
      "Prediction:\n",
      " [[ 0.52357954]\n",
      " [-0.69328415]\n",
      " [-0.33483574]\n",
      " [ 0.09802198]\n",
      " [-0.0936879 ]\n",
      " [-0.12798998]\n",
      " [-0.19704972]\n",
      " [-0.48252368]]\n",
      "39 Cost:  0.5889869 \n",
      "Prediction:\n",
      " [[ 0.52361435]\n",
      " [-0.69324857]\n",
      " [-0.3348065 ]\n",
      " [ 0.09804412]\n",
      " [-0.09366144]\n",
      " [-0.12796439]\n",
      " [-0.19703244]\n",
      " [-0.48250604]]\n",
      "40 Cost:  0.5889487 \n",
      "Prediction:\n",
      " [[ 0.5236491 ]\n",
      " [-0.693213  ]\n",
      " [-0.33477724]\n",
      " [ 0.09806626]\n",
      " [-0.09363494]\n",
      " [-0.12793873]\n",
      " [-0.19701514]\n",
      " [-0.48248848]]\n",
      "41 Cost:  0.5889106 \n",
      "Prediction:\n",
      " [[ 0.52368385]\n",
      " [-0.6931774 ]\n",
      " [-0.334748  ]\n",
      " [ 0.09808837]\n",
      " [-0.0936085 ]\n",
      " [-0.12791313]\n",
      " [-0.1969978 ]\n",
      " [-0.4824708 ]]\n",
      "42 Cost:  0.5888725 \n",
      "Prediction:\n",
      " [[ 0.52371866]\n",
      " [-0.6931419 ]\n",
      " [-0.3347187 ]\n",
      " [ 0.0981105 ]\n",
      " [-0.09358203]\n",
      " [-0.12788753]\n",
      " [-0.19698052]\n",
      " [-0.48245317]]\n",
      "43 Cost:  0.5888344 \n",
      "Prediction:\n",
      " [[ 0.5237534 ]\n",
      " [-0.6931062 ]\n",
      " [-0.33468947]\n",
      " [ 0.09813264]\n",
      " [-0.0935556 ]\n",
      " [-0.12786192]\n",
      " [-0.19696322]\n",
      " [-0.4824355 ]]\n",
      "44 Cost:  0.58879626 \n",
      "Prediction:\n",
      " [[ 0.52378815]\n",
      " [-0.69307065]\n",
      " [-0.3346602 ]\n",
      " [ 0.09815478]\n",
      " [-0.09352911]\n",
      " [-0.12783629]\n",
      " [-0.19694594]\n",
      " [-0.48241788]]\n",
      "45 Cost:  0.58875823 \n",
      "Prediction:\n",
      " [[ 0.52382296]\n",
      " [-0.6930351 ]\n",
      " [-0.33463097]\n",
      " [ 0.09817691]\n",
      " [-0.09350267]\n",
      " [-0.12781066]\n",
      " [-0.19692862]\n",
      " [-0.48240024]]\n",
      "46 Cost:  0.5887201 \n",
      "Prediction:\n",
      " [[ 0.5238577 ]\n",
      " [-0.6929995 ]\n",
      " [-0.33460167]\n",
      " [ 0.09819905]\n",
      " [-0.09347624]\n",
      " [-0.12778506]\n",
      " [-0.1969113 ]\n",
      " [-0.48238257]]\n",
      "47 Cost:  0.58868194 \n",
      "Prediction:\n",
      " [[ 0.52389246]\n",
      " [-0.6929639 ]\n",
      " [-0.33457243]\n",
      " [ 0.09822117]\n",
      " [-0.09344977]\n",
      " [-0.12775947]\n",
      " [-0.19689403]\n",
      " [-0.48236495]]\n",
      "48 Cost:  0.5886438 \n",
      "Prediction:\n",
      " [[ 0.5239273 ]\n",
      " [-0.69292825]\n",
      " [-0.33454323]\n",
      " [ 0.09824331]\n",
      " [-0.09342331]\n",
      " [-0.12773381]\n",
      " [-0.19687673]\n",
      " [-0.48234737]]\n",
      "49 Cost:  0.58860576 \n",
      "Prediction:\n",
      " [[ 0.523962  ]\n",
      " [-0.69289273]\n",
      " [-0.334514  ]\n",
      " [ 0.09826545]\n",
      " [-0.09339683]\n",
      " [-0.12770826]\n",
      " [-0.19685946]\n",
      " [-0.4823297 ]]\n",
      "50 Cost:  0.5885677 \n",
      "Prediction:\n",
      " [[ 0.5239968 ]\n",
      " [-0.692857  ]\n",
      " [-0.33448473]\n",
      " [ 0.09828753]\n",
      " [-0.09337042]\n",
      " [-0.12768263]\n",
      " [-0.19684215]\n",
      " [-0.48231205]]\n",
      "51 Cost:  0.5885295 \n",
      "Prediction:\n",
      " [[ 0.5240315 ]\n",
      " [-0.6928215 ]\n",
      " [-0.33445546]\n",
      " [ 0.09830967]\n",
      " [-0.09334399]\n",
      " [-0.12765704]\n",
      " [-0.19682483]\n",
      " [-0.48229444]]\n",
      "52 Cost:  0.58849144 \n",
      "Prediction:\n",
      " [[ 0.5240663 ]\n",
      " [-0.6927859 ]\n",
      " [-0.33442622]\n",
      " [ 0.0983318 ]\n",
      " [-0.09331752]\n",
      " [-0.12763144]\n",
      " [-0.19680755]\n",
      " [-0.4822768 ]]\n",
      "53 Cost:  0.5884534 \n",
      "Prediction:\n",
      " [[ 0.5241011 ]\n",
      " [-0.69275033]\n",
      " [-0.33439696]\n",
      " [ 0.09835393]\n",
      " [-0.09329106]\n",
      " [-0.1276058 ]\n",
      " [-0.19679026]\n",
      " [-0.4822592 ]]\n",
      "54 Cost:  0.58841527 \n",
      "Prediction:\n",
      " [[ 0.5241358 ]\n",
      " [-0.6927148 ]\n",
      " [-0.3343677 ]\n",
      " [ 0.09837607]\n",
      " [-0.09326458]\n",
      " [-0.1275802 ]\n",
      " [-0.19677295]\n",
      " [-0.48224154]]\n",
      "55 Cost:  0.5883772 \n",
      "Prediction:\n",
      " [[ 0.5241706 ]\n",
      " [-0.69267917]\n",
      " [-0.3343385 ]\n",
      " [ 0.09839819]\n",
      " [-0.09323817]\n",
      " [-0.1275546 ]\n",
      " [-0.19675568]\n",
      " [-0.48222396]]\n",
      "56 Cost:  0.5883391 \n",
      "Prediction:\n",
      " [[ 0.5242053 ]\n",
      " [-0.6926435 ]\n",
      " [-0.3343092 ]\n",
      " [ 0.09842032]\n",
      " [-0.09321172]\n",
      " [-0.127529  ]\n",
      " [-0.19673836]\n",
      " [-0.48220628]]\n",
      "57 Cost:  0.58830106 \n",
      "Prediction:\n",
      " [[ 0.52424014]\n",
      " [-0.692608  ]\n",
      " [-0.33427998]\n",
      " [ 0.09844245]\n",
      " [-0.09318526]\n",
      " [-0.12750337]\n",
      " [-0.19672109]\n",
      " [-0.48218864]]\n",
      "58 Cost:  0.5882629 \n",
      "Prediction:\n",
      " [[ 0.5242748 ]\n",
      " [-0.6925724 ]\n",
      " [-0.3342507 ]\n",
      " [ 0.09846456]\n",
      " [-0.09315884]\n",
      " [-0.12747777]\n",
      " [-0.1967038 ]\n",
      " [-0.482171  ]]\n",
      "59 Cost:  0.5882249 \n",
      "Prediction:\n",
      " [[ 0.5243096 ]\n",
      " [-0.6925369 ]\n",
      " [-0.33422145]\n",
      " [ 0.09848665]\n",
      " [-0.09313239]\n",
      " [-0.12745218]\n",
      " [-0.1966865 ]\n",
      " [-0.48215333]]\n",
      "60 Cost:  0.58818674 \n",
      "Prediction:\n",
      " [[ 0.5243443 ]\n",
      " [-0.69250125]\n",
      " [-0.33419228]\n",
      " [ 0.09850877]\n",
      " [-0.09310598]\n",
      " [-0.12742662]\n",
      " [-0.19666922]\n",
      " [-0.4821358 ]]\n",
      "61 Cost:  0.5881487 \n",
      "Prediction:\n",
      " [[ 0.524379  ]\n",
      " [-0.69246566]\n",
      " [-0.334163  ]\n",
      " [ 0.09853087]\n",
      " [-0.09307954]\n",
      " [-0.12740098]\n",
      " [-0.19665195]\n",
      " [-0.48211813]]\n",
      "62 Cost:  0.5881107 \n",
      "Prediction:\n",
      " [[ 0.52441376]\n",
      " [-0.69243026]\n",
      " [-0.33413374]\n",
      " [ 0.09855299]\n",
      " [-0.09305312]\n",
      " [-0.12737541]\n",
      " [-0.19663465]\n",
      " [-0.4821005 ]]\n",
      "63 Cost:  0.58807266 \n",
      "Prediction:\n",
      " [[ 0.5244485 ]\n",
      " [-0.6923946 ]\n",
      " [-0.33410457]\n",
      " [ 0.0985751 ]\n",
      " [-0.0930267 ]\n",
      " [-0.12734981]\n",
      " [-0.1966174 ]\n",
      " [-0.48208284]]\n",
      "64 Cost:  0.58803463 \n",
      "Prediction:\n",
      " [[ 0.5244832 ]\n",
      " [-0.6923591 ]\n",
      " [-0.33407533]\n",
      " [ 0.09859723]\n",
      " [-0.09300025]\n",
      " [-0.12732424]\n",
      " [-0.19660008]\n",
      " [-0.48206526]]\n",
      "65 Cost:  0.5879966 \n",
      "Prediction:\n",
      " [[ 0.52451795]\n",
      " [-0.69232345]\n",
      " [-0.3340461 ]\n",
      " [ 0.09861933]\n",
      " [-0.09297381]\n",
      " [-0.12729864]\n",
      " [-0.19658281]\n",
      " [-0.4820476 ]]\n",
      "66 Cost:  0.5879586 \n",
      "Prediction:\n",
      " [[ 0.5245526 ]\n",
      " [-0.69228804]\n",
      " [-0.33401686]\n",
      " [ 0.0986414 ]\n",
      " [-0.09294739]\n",
      " [-0.12727307]\n",
      " [-0.19656551]\n",
      " [-0.48203   ]]\n",
      "67 Cost:  0.58792055 \n",
      "Prediction:\n",
      " [[ 0.5245873 ]\n",
      " [-0.69225246]\n",
      " [-0.33398768]\n",
      " [ 0.09866349]\n",
      " [-0.09292105]\n",
      " [-0.12724756]\n",
      " [-0.19654825]\n",
      " [-0.48201242]]\n",
      "68 Cost:  0.5878826 \n",
      "Prediction:\n",
      " [[ 0.52462196]\n",
      " [-0.69221693]\n",
      " [-0.3339585 ]\n",
      " [ 0.09868558]\n",
      " [-0.09289461]\n",
      " [-0.12722194]\n",
      " [-0.19653097]\n",
      " [-0.48199478]]\n",
      "69 Cost:  0.5878446 \n",
      "Prediction:\n",
      " [[ 0.5246566 ]\n",
      " [-0.6921814 ]\n",
      " [-0.3339293 ]\n",
      " [ 0.09870767]\n",
      " [-0.09286822]\n",
      " [-0.12719642]\n",
      " [-0.19651367]\n",
      " [-0.48197713]]\n",
      "70 Cost:  0.5878066 \n",
      "Prediction:\n",
      " [[ 0.5246913 ]\n",
      " [-0.6921459 ]\n",
      " [-0.3339001 ]\n",
      " [ 0.09872976]\n",
      " [-0.09284185]\n",
      " [-0.12717088]\n",
      " [-0.1964964 ]\n",
      " [-0.48195952]]\n",
      "71 Cost:  0.58776855 \n",
      "Prediction:\n",
      " [[ 0.5247259 ]\n",
      " [-0.69211036]\n",
      " [-0.33387092]\n",
      " [ 0.09875182]\n",
      " [-0.09281541]\n",
      " [-0.12714529]\n",
      " [-0.19647914]\n",
      " [-0.48194188]]\n",
      "72 Cost:  0.5877305 \n",
      "Prediction:\n",
      " [[ 0.5247606 ]\n",
      " [-0.69207484]\n",
      " [-0.33384168]\n",
      " [ 0.09877393]\n",
      " [-0.09278902]\n",
      " [-0.12711973]\n",
      " [-0.19646186]\n",
      " [-0.48192424]]\n",
      "73 Cost:  0.58769256 \n",
      "Prediction:\n",
      " [[ 0.5247953 ]\n",
      " [-0.6920394 ]\n",
      " [-0.3338125 ]\n",
      " [ 0.098796  ]\n",
      " [-0.09276262]\n",
      " [-0.12709415]\n",
      " [-0.19644457]\n",
      " [-0.4819067 ]]\n",
      "74 Cost:  0.58765453 \n",
      "Prediction:\n",
      " [[ 0.5248299 ]\n",
      " [-0.69200385]\n",
      " [-0.3337833 ]\n",
      " [ 0.09881809]\n",
      " [-0.09273624]\n",
      " [-0.12706861]\n",
      " [-0.19642732]\n",
      " [-0.48188907]]\n",
      "75 Cost:  0.58761656 \n",
      "Prediction:\n",
      " [[ 0.5248646 ]\n",
      " [-0.6919683 ]\n",
      " [-0.33375412]\n",
      " [ 0.09884016]\n",
      " [-0.09270987]\n",
      " [-0.12704308]\n",
      " [-0.19641003]\n",
      " [-0.48187143]]\n",
      "76 Cost:  0.5875786 \n",
      "Prediction:\n",
      " [[ 0.52489924]\n",
      " [-0.6919328 ]\n",
      " [-0.33372492]\n",
      " [ 0.09886226]\n",
      " [-0.09268343]\n",
      " [-0.12701748]\n",
      " [-0.19639274]\n",
      " [-0.48185378]]\n",
      "77 Cost:  0.5875406 \n",
      "Prediction:\n",
      " [[ 0.52493393]\n",
      " [-0.69189733]\n",
      " [-0.33369574]\n",
      " [ 0.09888435]\n",
      " [-0.09265705]\n",
      " [-0.12699196]\n",
      " [-0.19637546]\n",
      " [-0.4818362 ]]\n",
      "78 Cost:  0.5875026 \n",
      "Prediction:\n",
      " [[ 0.52496856]\n",
      " [-0.69186175]\n",
      " [-0.33366656]\n",
      " [ 0.09890641]\n",
      " [-0.09263065]\n",
      " [-0.12696639]\n",
      " [-0.1963582 ]\n",
      " [-0.48181856]]\n",
      "79 Cost:  0.58746463 \n",
      "Prediction:\n",
      " [[ 0.52500325]\n",
      " [-0.6918263 ]\n",
      " [-0.33363736]\n",
      " [ 0.09892851]\n",
      " [-0.09260427]\n",
      " [-0.12694082]\n",
      " [-0.19634092]\n",
      " [-0.48180097]]\n",
      "80 Cost:  0.58742666 \n",
      "Prediction:\n",
      " [[ 0.5250379 ]\n",
      " [-0.6917908 ]\n",
      " [-0.33360812]\n",
      " [ 0.0989506 ]\n",
      " [-0.09257787]\n",
      " [-0.12691528]\n",
      " [-0.19632366]\n",
      " [-0.4817834 ]]\n",
      "81 Cost:  0.58738875 \n",
      "Prediction:\n",
      " [[ 0.5250726 ]\n",
      " [-0.6917553 ]\n",
      " [-0.33357897]\n",
      " [ 0.09897267]\n",
      " [-0.09255146]\n",
      " [-0.12688969]\n",
      " [-0.19630638]\n",
      " [-0.48176575]]\n",
      "82 Cost:  0.5873508 \n",
      "Prediction:\n",
      " [[ 0.5251072 ]\n",
      " [-0.6917198 ]\n",
      " [-0.33354977]\n",
      " [ 0.09899473]\n",
      " [-0.09252509]\n",
      " [-0.12686417]\n",
      " [-0.1962891 ]\n",
      " [-0.48174816]]\n",
      "83 Cost:  0.5873128 \n",
      "Prediction:\n",
      " [[ 0.5251419 ]\n",
      " [-0.6916843 ]\n",
      " [-0.3335206 ]\n",
      " [ 0.09901685]\n",
      " [-0.09249869]\n",
      " [-0.12683862]\n",
      " [-0.1962718 ]\n",
      " [-0.48173052]]\n",
      "84 Cost:  0.58727485 \n",
      "Prediction:\n",
      " [[ 0.5251765 ]\n",
      " [-0.6916488 ]\n",
      " [-0.33349138]\n",
      " [ 0.09903891]\n",
      " [-0.09247231]\n",
      " [-0.12681304]\n",
      " [-0.19625455]\n",
      " [-0.48171288]]\n",
      "85 Cost:  0.5872369 \n",
      "Prediction:\n",
      " [[ 0.52521116]\n",
      " [-0.69161326]\n",
      " [-0.3334622 ]\n",
      " [ 0.09906097]\n",
      " [-0.09244591]\n",
      " [-0.12678753]\n",
      " [-0.19623727]\n",
      " [-0.48169523]]\n",
      "86 Cost:  0.58719885 \n",
      "Prediction:\n",
      " [[ 0.52524585]\n",
      " [-0.69157773]\n",
      " [-0.333433  ]\n",
      " [ 0.09908305]\n",
      " [-0.09241951]\n",
      " [-0.12676194]\n",
      " [-0.19622001]\n",
      " [-0.4816777 ]]\n",
      "87 Cost:  0.58716094 \n",
      "Prediction:\n",
      " [[ 0.5252805 ]\n",
      " [-0.6915422 ]\n",
      " [-0.3334038 ]\n",
      " [ 0.09910515]\n",
      " [-0.09239314]\n",
      " [-0.12673642]\n",
      " [-0.19620274]\n",
      " [-0.48166007]]\n",
      "88 Cost:  0.587123 \n",
      "Prediction:\n",
      " [[ 0.52531517]\n",
      " [-0.6915067 ]\n",
      " [-0.33337462]\n",
      " [ 0.09912721]\n",
      " [-0.09236674]\n",
      " [-0.12671089]\n",
      " [-0.19618548]\n",
      " [-0.48164243]]\n",
      "89 Cost:  0.587085 \n",
      "Prediction:\n",
      " [[ 0.5253498 ]\n",
      " [-0.69147116]\n",
      " [-0.33334547]\n",
      " [ 0.09914929]\n",
      " [-0.09234034]\n",
      " [-0.12668529]\n",
      " [-0.1961682 ]\n",
      " [-0.48162487]]\n",
      "90 Cost:  0.58704704 \n",
      "Prediction:\n",
      " [[ 0.5253845 ]\n",
      " [-0.69143564]\n",
      " [-0.33331624]\n",
      " [ 0.09917136]\n",
      " [-0.09231398]\n",
      " [-0.12665978]\n",
      " [-0.19615093]\n",
      " [-0.48160723]]\n",
      "91 Cost:  0.5870091 \n",
      "Prediction:\n",
      " [[ 0.5254191 ]\n",
      " [-0.6914002 ]\n",
      " [-0.33328706]\n",
      " [ 0.09919345]\n",
      " [-0.09228757]\n",
      " [-0.12663418]\n",
      " [-0.19613366]\n",
      " [-0.4815896 ]]\n",
      "92 Cost:  0.58697116 \n",
      "Prediction:\n",
      " [[ 0.52545375]\n",
      " [-0.69136477]\n",
      " [-0.33325788]\n",
      " [ 0.09921552]\n",
      " [-0.09226116]\n",
      " [-0.12660865]\n",
      " [-0.1961164 ]\n",
      " [-0.481572  ]]\n",
      "93 Cost:  0.5869332 \n",
      "Prediction:\n",
      " [[ 0.52548844]\n",
      " [-0.6913291 ]\n",
      " [-0.3332287 ]\n",
      " [ 0.09923759]\n",
      " [-0.09223479]\n",
      " [-0.12658313]\n",
      " [-0.19609912]\n",
      " [-0.48155445]]\n",
      "94 Cost:  0.5868953 \n",
      "Prediction:\n",
      " [[ 0.52552307]\n",
      " [-0.6912937 ]\n",
      " [-0.33319956]\n",
      " [ 0.09925965]\n",
      " [-0.09220839]\n",
      " [-0.12655754]\n",
      " [-0.19608188]\n",
      " [-0.4815368 ]]\n",
      "95 Cost:  0.5868573 \n",
      "Prediction:\n",
      " [[ 0.5255577 ]\n",
      " [-0.6912582 ]\n",
      " [-0.33317038]\n",
      " [ 0.09928174]\n",
      " [-0.092182  ]\n",
      " [-0.12653202]\n",
      " [-0.19606459]\n",
      " [-0.48151922]]\n",
      "96 Cost:  0.5868194 \n",
      "Prediction:\n",
      " [[ 0.5255924 ]\n",
      " [-0.69122267]\n",
      " [-0.3331412 ]\n",
      " [ 0.09930381]\n",
      " [-0.09215567]\n",
      " [-0.12650646]\n",
      " [-0.19604734]\n",
      " [-0.48150158]]\n",
      "97 Cost:  0.58678144 \n",
      "Prediction:\n",
      " [[ 0.525627  ]\n",
      " [-0.69118714]\n",
      " [-0.33311203]\n",
      " [ 0.09932587]\n",
      " [-0.09212924]\n",
      " [-0.1264809 ]\n",
      " [-0.19603007]\n",
      " [-0.48148397]]\n",
      "98 Cost:  0.58674353 \n",
      "Prediction:\n",
      " [[ 0.5256617 ]\n",
      " [-0.69115174]\n",
      " [-0.3330828 ]\n",
      " [ 0.09934797]\n",
      " [-0.09210287]\n",
      " [-0.12645538]\n",
      " [-0.1960128 ]\n",
      " [-0.48146638]]\n",
      "99 Cost:  0.58670557 \n",
      "Prediction:\n",
      " [[ 0.52569634]\n",
      " [-0.6911162 ]\n",
      " [-0.33305365]\n",
      " [ 0.09937003]\n",
      " [-0.09207648]\n",
      " [-0.12642981]\n",
      " [-0.19599555]\n",
      " [-0.4814488 ]]\n",
      "100 Cost:  0.58666766 \n",
      "Prediction:\n",
      " [[ 0.52573097]\n",
      " [-0.6910807 ]\n",
      " [-0.33302447]\n",
      " [ 0.09939209]\n",
      " [-0.09205008]\n",
      " [-0.12640429]\n",
      " [-0.19597825]\n",
      " [-0.4814312 ]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "xy = np.array([[828.659973, 833.450012, 908100, 828.349976, 831.659973],\n",
    "               [823.02002, 828.070007, 1828100, 821.655029, 828.070007],\n",
    "               [819.929993, 824.400024, 1438100, 818.97998, 824.159973],\n",
    "               [816, 820.958984, 1008100, 815.48999, 819.23999],\n",
    "               [819.359985, 823, 1188100, 818.469971, 818.97998],\n",
    "               [819, 823, 1198100, 816, 820.450012],\n",
    "               [811.700012, 815.25, 1098100, 809.780029, 813.669983],\n",
    "               [809.51001, 816.659973, 1398100, 804.539978, 809.559998]])\n",
    "\n",
    "x_data = xy[:, 0:-1]\n",
    "y_data = xy[:, [-1]]\n",
    "\n",
    "W = tf.Variable(tf.random.normal([4, 1]), name='weight')\n",
    "b = tf.Variable(tf.random.normal([1]), name='bias')\n",
    "\n",
    "def model(X):\n",
    "    return tf.matmul(X, W) + b\n",
    "\n",
    "def loss(Y, hypothesis):\n",
    "    return tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "\n",
    "optimizer = tf.optimizers.SGD(learning_rate=1e-5)\n",
    "\n",
    "def train(X, Y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        hypothesis = model(X)\n",
    "        cost = loss(Y, hypothesis)\n",
    "    gradients = tape.gradient(cost, [W, b])\n",
    "    optimizer.apply_gradients(zip(gradients, [W, b]))\n",
    "\n",
    "# Min-Max Normalization\n",
    "def min_max_normalize(dataset):\n",
    "    min_value = np.min(dataset, axis=0)\n",
    "    max_value = np.max(dataset, axis=0)\n",
    "    \n",
    "    return (dataset - min_value) / (max_value - min_value)\n",
    "\n",
    "x_data_normalized = min_max_normalize(x_data)\n",
    "y_data_normalized = min_max_normalize(y_data)\n",
    "\n",
    "X = tf.constant(x_data_normalized, dtype=tf.float32)\n",
    "Y = tf.constant(y_data_normalized, dtype=tf.float32)\n",
    "\n",
    "for step in range(101):\n",
    "    train(X, Y)\n",
    "    cost_val = loss(Y, model(X))\n",
    "    hy_val = model(X)\n",
    "    print(step, \"Cost: \", cost_val.numpy(), \"\\nPrediction:\\n\", hy_val.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001, Cost: 1.253804140\n",
      "Epoch: 0002, Cost: 0.566553944\n",
      "Epoch: 0003, Cost: 0.478168053\n",
      "Epoch: 0004, Cost: 0.434260539\n",
      "Epoch: 0005, Cost: 0.406304750\n",
      "Epoch: 0006, Cost: 0.386369006\n",
      "Epoch: 0007, Cost: 0.371190658\n",
      "Epoch: 0008, Cost: 0.359123811\n",
      "Epoch: 0009, Cost: 0.349229012\n",
      "Epoch: 0010, Cost: 0.340923515\n",
      "Epoch: 0011, Cost: 0.333823285\n",
      "Epoch: 0012, Cost: 0.327663182\n",
      "Epoch: 0013, Cost: 0.322253250\n",
      "Epoch: 0014, Cost: 0.317453251\n",
      "Epoch: 0015, Cost: 0.313157052\n",
      "Learning finished\n",
      "Accuracy:  0.9059\n",
      "Label:  [1]\n",
      "Prediction:  [1]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "# 데이터셋을 로드하고 훈련 및 테스트 세트로 분할\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# 정규화\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "# 원-핫 인코딩\n",
    "y_train = tf.one_hot(y_train, depth=10, dtype=tf.float64)\n",
    "y_test = tf.one_hot(y_test, depth=10, dtype=tf.float64)\n",
    "\n",
    "# MNIST 이미지는 28x28 크기이므로 784 길이의 벡터로 변환\n",
    "x_train = x_train.reshape(-1, 28*28)\n",
    "x_test = x_test.reshape(-1, 28*28)\n",
    "\n",
    "W = tf.Variable(tf.random.uniform([784, 10], minval=-1., maxval=1., dtype=tf.float64))\n",
    "b = tf.Variable(tf.zeros([10], dtype=tf.float64))\n",
    "\n",
    "# 가설 설정 (소프트맥스 사용)\n",
    "@tf.function\n",
    "def model(X):\n",
    "    return tf.nn.softmax(tf.matmul(X, W) + b)\n",
    "\n",
    "# 비용함수 (크로스 엔트로피 사용)\n",
    "def cost(X, Y):\n",
    "    return -tf.reduce_mean(tf.reduce_sum(Y * tf.math.log(model(X)), axis=1))\n",
    "\n",
    "def train(X, Y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        c = cost(X, Y)\n",
    "        \n",
    "    gradients = tape.gradient(c, [W, b])\n",
    "    optimizer = tf.optimizers.SGD(learning_rate=0.1)\n",
    "    optimizer.apply_gradients(zip(gradients, [W, b]))\n",
    "\n",
    "# 모델 테스트\n",
    "def is_correct(X, Y):\n",
    "    return tf.equal(tf.argmax(model(X), 1), tf.argmax(Y, 1))\n",
    "\n",
    "# 정확도 계산\n",
    "def accuracy(X, Y):\n",
    "    return tf.reduce_mean(tf.cast(is_correct(X, Y), tf.float32))\n",
    "\n",
    "# 매개변수\n",
    "num_epochs = 15\n",
    "batch_size = 100\n",
    "total_batch = x_train.shape[0] // batch_size\n",
    "\n",
    "# 훈련 사이클\n",
    "for epoch in range(num_epochs):\n",
    "    avg_cost = 0\n",
    "\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = x_train[i * batch_size:(i+1) * batch_size], y_train[i * batch_size:(i+1) * batch_size]\n",
    "        train(batch_xs, batch_ys)\n",
    "        avg_cost += cost(batch_xs, batch_ys) / total_batch\n",
    "\n",
    "    print(\"Epoch: {:04d}, Cost: {:.9f}\".format(epoch + 1, avg_cost))\n",
    "\n",
    "print(\"Learning finished\")\n",
    "\n",
    "# 테스트 셋을 사용하여 모델 테스트 및 정확도 계산\n",
    "print(\"Accuracy: \", accuracy(x_test, y_test).numpy())\n",
    "\n",
    "# 하나의 예측값 가져오기\n",
    "r = random.randint(0, x_test.shape[0] - 1)\n",
    "print(\"Label: \", tf.argmax(y_test[r : r + 1], 1).numpy())\n",
    "print(\"Prediction: \", tf.argmax(model(x_test[r : r + 1]), 1).numpy())\n",
    "\n",
    "# 예측한 이미지 출력\n",
    "plt.imshow(x_test[r : r + 1].reshape(28, 28), cmap=\"Greys\", interpolation=\"nearest\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
